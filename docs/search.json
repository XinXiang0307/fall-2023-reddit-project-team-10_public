[
  {
    "objectID": "eda.html#submission-data",
    "href": "eda.html#submission-data",
    "title": "Milestone 1 EDA",
    "section": "1. Submission Data",
    "text": "1. Submission Data\n\n1.1 Preliminary Cleaning\nThe submission dataset is refined by renaming columns for better identification, such as prefixing them with ‘submission_’. It is then filtered to retain entries solely from the ‘MBTI’ subreddit, ensuring relevance to the topic of interest. The final step in this cleaning phase involves selecting key columns that are critical for analysis, thereby simplifying the dataset and preparing it for the next stages of the project.\n\n\nCode\nfrom pyspark.sql.functions import col, substring\n\n# Rename the columns\nsubmissions_df = submissions_df.withColumnRenamed(\"author\", \"submission_author\") \\\n                   .withColumnRenamed(\"title\", \"submission_title\") \\\n                   .withColumnRenamed(\"score\", \"submission_score\") \\\n                   .withColumnRenamed(\"created_utc\", \"submission_created_date\") \\\n                   .withColumnRenamed(\"year\", \"submission_year\") \\\n                   .withColumnRenamed(\"month\", \"submission_month\") \\\n                   .withColumnRenamed(\"self_text\", \"submission_self_text\")\n\n# filter by subreddit\nsubmissions_df = submissions_df.filter(col(\"subreddit\") == \"mbti\")\n\n# subset data\nsubmissions_df = submissions_df.select(\"id\",\"submission_author\",\"num_crossposts\",\"num_comments\",\"submission_score\",\"submission_title\",\"submission_created_date\",\"submission_year\",\"submission_month\")\n\n\n\n\n1.2 Create “Mbti_type_related” Column Using Regex\nIn this step, a regular expression pattern was crafted to match any of the sixteen MBTI personality types mentioned in subreddit post titles. A custom extraction function was defined to search for these types and aggregate any found into the new mbti_type_related column, all in uppercase for uniformity. Titles featuring multiple MBTI types have them all listed in this column, separated by commas. For instance, a post titled “How do INFP and INTJ get along?” would yield “INFP, INTJ” in this column. Posts that don’t reference specific MBTI types are assigned the value ‘general’, ensuring that every post is categorized for later analysis. This step is crucial for our goal of analyzing post statistics relative to MBTI orientations in subreddit discussions.\n\n\nCode\nfrom pyspark.sql.functions import udf, when\nfrom pyspark.sql.types import StringType\nimport re\n\n# List of personality types in uppercase\npersonality_types = [\"ESTJ\", \"ISTJ\", \"INFP\", \"ENFP\", \"INTJ\", \"ENTJ\", \"INTP\", \"ENTP\",\n                    \"ESFJ\", \"ISFJ\", \"ENFJ\", \"INFJ\", \"ESFP\", \"ISFP\", \"ISTP\", \"ESTP\"]\n\n# Convert the list to a regex pattern with case-insensitive flag\npattern = \"(?i)\\\\b(\" + \"|\".join(personality_types) + \")\\\\b\"\n\n# Define UDF to extract all matches\ndef extract_all_types(title):\n    matches = re.findall(pattern, title, re.IGNORECASE)\n    # Convert matches to uppercase\n    matches_upper = [match.upper() for match in matches]\n    return ', '.join(matches_upper)\n\nextract_all_types_udf = udf(extract_all_types, StringType())\n\n# Apply UDF to get all MBTI types\nsubmission_df = submissions_df.withColumn(\"mbti_type_related_temp\", extract_all_types_udf(col(\"submission_title\")))\n\n# Set 'mbti_type_related' to 'general' for empty matches\nsubmission_df = submission_df.withColumn(\"mbti_type_related\",\n                                         when(col(\"mbti_type_related_temp\") == \"\", \"general\")\n                                         .otherwise(col(\"mbti_type_related_temp\")))\n\n# Drop the temporary column\nsubmission_df = submission_df.drop(\"mbti_type_related_temp\")\n\n\n\n\n\n\n\n\n\n\n\nsubmission_title\nmbti_type_related\n\n\n\n\nFe-Ti vs Fi-Te??? :/\ngeneral\n\n\nI suck at finding my MBTI type\ngeneral\n\n\nI keep getting 4w5 but i am an entp. are the two things correlated or not? (before u say anything, i am not an enfp, already thought about it and confirmed i am not)\nENTP, ENFP\n\n\nis the combination of estj 2w3 possible?\nESTJ\n\n\nNeed help with 8 Functions\ngeneral\n\n\n\n\n\n\n\n1.3 Final Submission Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nsubmission_author\nnum_crossposts\nnum_comments\nsubmission_score\nsubmission_title\nsubmission_created_date\nsubmission_year\nsubmission_month\nmbti_type_related\n\n\n\n\nui1lbb\nForbidden_Crisp\n0\n4\n2\nHelp me type my BF, pls!\n2022-05-04 08:04:20\n2022\n5\ngeneral\n\n\nui1nfk\nAcceptable-Elk2871\n0\n2\n2\nPerfectionism in Ti vs Te users\n2022-05-04 08:09:02\n2022\n5\ngeneral"
  },
  {
    "objectID": "eda.html#comment-data",
    "href": "eda.html#comment-data",
    "title": "Milestone 1 EDA",
    "section": "2. Comment Data",
    "text": "2. Comment Data\n\n2.1 Preliminary Cleaning\nThe comment dataset undergoes a cleaning process where columns are renamed with a ‘comment_’ prefix to ensure clarity and consistency. Only essential columns such as submission identifiers, author information, comment content, and associated metadata are retained. This streamlined dataset is then prepared for targeted analysis within the ‘MBTI’ subreddit community.\n\n\nCode\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\n# Rename the columns\ncomments_df = comments_df.withColumnRenamed(\"author\", \"comment_author\") \\\n                   .withColumnRenamed(\"body\", \"comment_text\") \\\n                   .withColumnRenamed(\"score\", \"comment_score\")\\\n                   .withColumnRenamed(\"controversiality\", \"comment_controversiality\")\n\n# select useful columns\ncomments_df = comments_df.select(\"sub_id\",\"comment_author\", \"comment_text\", \"link_id\",\"comment_score\",\"comment_controversiality\",\"reply_to\",\"year\",\"month\")\n\n\n\n\n2.2 Create “reply_to” Column\nDuring the data cleaning phase, a new ‘reply_to’ column is derived from the ‘parent_id’ field to classify comments within the conversation thread. This column captures the first two characters of ‘parent_id’, distinguishing top-level comments directed at the submission (‘T3’) from those replying to other comments (‘T1’). The ‘reply_to’ column is integral to upcoming analyses, where it will enable a detailed examination of comment lengths and patterns, segmented by their position in the discussion hierarchy.\n\n\nCode\n# Create a new column 'reply_to' with the first two characters from 'parent_id'\ncomments_df = comments_df.withColumn(\"reply_to\", substring(col(\"parent_id\"), 1, 2))\ncomments_df = comments_df.filter(col(\"subreddit\") == \"mbti\")\n\n\n\n\n2.3 Final Comment Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsub_id\ncomment_author\ncomment_text\nlink_id\ncomment_score\ncomment_controversiality\nreply_to\nyear\nmonth\n\n\n\n\ntkhm8b\n-Yoyo_playz-\nI’m sure most of the people did it for fun :)\nt3_tkhm8b\n2\n0\nt3\n2022\n3\n\n\ntk9n8u\nTSE_Jazz\nAnyone who relates to this is probably mistyped\nt3_tk9n8u\n21\n0\nt3\n2022\n3"
  },
  {
    "objectID": "eda.html#external-data",
    "href": "eda.html#external-data",
    "title": "Milestone 1 EDA",
    "section": "3. External Data",
    "text": "3. External Data\n\n3.1 PersonalityPost dataset\nThe project taps into a comprehensive Reddit dataset to explore the vast array of conversations about the Myers-Briggs Type Indicator (MBTI) within the online community. Although the dataset sheds light on the general topics that captivate those interested in MBTI, it stops short of linking the users’ posts to their specific MBTI types due to the absence of such personal data.\nTo address this limitation, an auxiliary dataset from the PersonalityCafe forum, which is abundant in self-declared MBTI types and the users’ written content, has been integrated.\nThe PersonalityCafe dataset is extensive, featuring over 8,600 records. Each entry comprises:\n\nThe participant’s four-letter MBTI type.\nA collection of the individual’s most recent 50 posts, with each one separated by a triplet of pipe characters (“|||”).\n\nThis combined dataset provides a refined lens for analyzing the relationship between MBTI types and the distinct topics they discuss.\n\nPreliminary Cleaning\nThe preliminary data cleaning process involves importing the MBTI dataset and segmenting concatenated posts into distinct records. Each post, separated by the “|||” delimiter, is extracted and mapped to the respective MBTI type, creating an expanded DataFrame. This step ensures the dataset is well-organized and primed for further analysis.\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv('./data/csv/mbti_post.csv')\n\n# extract post for each personality\n# A section of each of the last 50 things they have posted (Each entry separated by \"|||\" (3 pipe characters))\n\n# Create a new DataFrame for the expanded posts\nposts_expanded = pd.DataFrame(columns=['type', 'post'])\n\n# Loop over the rows and split the posts, then append to the new DataFrame\nfor index, row in df.iterrows():\n    # Split the posts into a list using the \"|||\" separator\n    posts = row['posts'].split('|||')\n    # Create a temporary DataFrame with the split posts and the corresponding type\n    temp_df = pd.DataFrame({'type': row['type'], 'post': posts})\n    # Append the temporary DataFrame to the expanded DataFrame\n    posts_expanded = pd.concat([posts_expanded, temp_df], ignore_index=True)\n\n\n\n\nFinal PersonalityPost Data\n\n\nCode\nimport pandas as pd\n# check the dataframe\nposts_expanded = pd.read_csv('../data/csv/clean_post.csv')\nposts_expanded.head()\n\n\n\n\n\n\n  \n    \n      \n      type\n      post\n    \n  \n  \n    \n      0\n      INFJ\n      enfp and intj moments    sportscenter not top ...\n    \n    \n      1\n      INFJ\n      What has been the most life-changing experienc...\n    \n    \n      2\n      INFJ\n      On repeat for most of today.\n    \n    \n      3\n      INFJ\n      May the PerC Experience immerse you.\n    \n    \n      4\n      INFJ\n      The last thing my INFJ friend posted on his fa...\n    \n  \n\n\n\n\n\n\n\n3.2 Health Condition Personality dataset\nOccupational back pain is a well-known condition that commonly affects the working population, resulting in disability, health care utilization, and significant socioeconomic burden. Although the causes of occupational pain remain largely unresolved, there is anecdotal evidence that personality and posture contribute to long-term pain management.\nTherefore, when we study MBTI personality types, we can conduct some research on health issues extended by MBTI to confirm some fresh ideas. In this regard, we conducted EDA from this Kaggle dataset Health Condition Personality dataset to try to find the correlation between them.\nThe dimensions of this original data are (97, 20). Although the data volume is small, after looking at the raw data, we can find that there are many directions that can be explored. Therefore, we decided to explore and model EDA and part of Machine learning on this data.\nAbout the data\n\nThe results from the personality inventory (MBTI) came in the form of a combination of letters [Extraversion, Introversion, Sensing, Intuition, Thinking, Feeling, Judging, and Perceiving], and numbers, 1–26.\nThe pain scale data consisted of a number between 0(low) to 10 (high).\nPosture - ideal posture (A), kyphosis-lordosis (B), flat back (C), sway-back (D)\n\n\nHealth Condition Cleaning\nDuring the cleaning process, we will find that the data has too many pain details, which is not conducive to analysis, so we merge the four columns about the degree of pain and create a new ‘SUM PAIN’ column. Use this column to create a unified pain rating for the body.\nThe second is about posture classification, mapping the original meaning instead of expressing it as A, B, C, and D. In this case, the EDA part will be more reader-friendly.\n\n\nCode\nmbti_health_df = pd.read_csv(\"../data/csv/mbti_health.csv\")\n# create a new column to sum the pain of each people.\nmbti_health_df['SUM PAIN'] = mbti_health_df[['PAIN 1', 'PAIN 2', 'PAIN 3', 'PAIN 4']].sum(axis=1)\n\n## bese on the data describtion, A,B,C,D represent the posture conditions below\nposture_conditions = {\n    'A': 'Ideal posture',\n    'B': 'Kyphosis-lordosis',\n    'C': 'Flat back',\n    'D': 'Sway-back'\n}\nmbti_health_df['POSTURE DETAIL'] = mbti_health_df['POSTURE'].map(posture_conditions)\n\n\n\n\nFinal Health-MBTI Data\n\n\nCode\nmbti_health_df.head()\n\n\n\n\n\n\n  \n    \n      \n      S No\n      AGE\n      HEIGHT\n      WEIGHT\n      SEX\n      ACTIVITY LEVEL\n      PAIN 1\n      PAIN 2\n      PAIN 3\n      PAIN 4\n      ...\n      I\n      S\n      N\n      T\n      F\n      J\n      P\n      POSTURE\n      SUM PAIN\n      POSTURE DETAIL\n    \n  \n  \n    \n      0\n      1\n      53\n      62\n      125\n      Female\n      Low\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      3\n      17\n      9\n      9\n      13\n      18\n      4\n      A\n      0.0\n      Ideal posture\n    \n    \n      1\n      2\n      52\n      69\n      157\n      Male\n      High\n      7.0\n      8.0\n      5.0\n      3.0\n      ...\n      15\n      14\n      12\n      21\n      3\n      13\n      9\n      B\n      23.0\n      Kyphosis-lordosis\n    \n    \n      2\n      3\n      30\n      69\n      200\n      Male\n      High\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      6\n      16\n      10\n      15\n      9\n      12\n      10\n      A\n      0.0\n      Ideal posture\n    \n    \n      3\n      4\n      51\n      66\n      175\n      Male\n      Moderate\n      9.5\n      9.5\n      9.5\n      1.5\n      ...\n      15\n      21\n      5\n      13\n      11\n      19\n      3\n      D\n      30.0\n      Sway-back\n    \n    \n      4\n      5\n      45\n      63\n      199\n      Female\n      Moderate\n      4.0\n      5.0\n      2.0\n      2.0\n      ...\n      7\n      20\n      6\n      9\n      15\n      16\n      6\n      A\n      13.0\n      Ideal posture\n    \n  \n\n5 rows × 22 columns"
  },
  {
    "objectID": "eda.html#cleaned-data-columns",
    "href": "eda.html#cleaned-data-columns",
    "title": "Milestone 1 EDA",
    "section": "4. Cleaned Data Columns",
    "text": "4. Cleaned Data Columns\n\n\n\n\n\n\nComment Data Columns\n\n\n\n\n\n\n\nSubmission Data Columns"
  },
  {
    "objectID": "eda.html#posting-behavior-analysis-for-16-mbti-personality-types",
    "href": "eda.html#posting-behavior-analysis-for-16-mbti-personality-types",
    "title": "Milestone 1 EDA",
    "section": "4.1 Posting Behavior Analysis for 16 MBTI Personality Types",
    "text": "4.1 Posting Behavior Analysis for 16 MBTI Personality Types\nAfter the initial data structuring, the dataset was grouped by the MBTI personality type, allowing for the calculation of the total number of posts per type, the average length of posts, and the relative percentage of posts each type represents in the dataset. The ‘number_of_posts’ column was aggregated for count, and a lambda function was applied to calculate the ‘average_length_of_posts’. The resulting ‘percentage_of_posts’ column was then derived by dividing the count of posts per type by the total number of posts, offering a proportionate view of the data. This organized summary dataframe was subsequently sorted by the number of posts in descending order, enabling a clear, quantitative comparison across the 16 MBTI personality types and preparing the data for insightful analysis and visualization.\n\n\nCode\n# Group by the 'type' and then perform the aggregation\nsummary_df = posts_expanded.groupby('type').agg(\n    number_of_posts=pd.NamedAgg(column='post', aggfunc='count'),\n    average_length_of_posts=pd.NamedAgg(column='post', aggfunc=lambda x: x.str.len().mean())\n).reset_index()\n\n# add percentage\ntotal_posts = summary_df['number_of_posts'].sum()\nsummary_df['percentage_of_posts'] = summary_df['number_of_posts'] / total_posts\n\n# sort the data\nsummary_df = summary_df.sort_values(by='number_of_posts', ascending=False).reset_index(drop=True)\n\n# check\nsummary_df\n# .to_csv('../data/csv/mbti_post_summary_df.csv')\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype\nnumber_of_posts\naverage_length_of_posts\npercentage_of_posts\n\n\n\n\nINFP\n89796\n146.361\n0.212362\n\n\nINFJ\n72105\n150.111\n0.170523\n\n\nINTP\n63359\n144.327\n0.14984\n\n\nINTJ\n52471\n145.21\n0.12409\n\n\nENTP\n33761\n141.349\n0.0798425\n\n\nENFP\n32769\n147.837\n0.0774965\n\n\nISTP\n16498\n138.285\n0.0390167\n\n\nISFP\n13000\n137.921\n0.0307441\n\n\nENTJ\n11273\n145.086\n0.0266599\n\n\nISTJ\n9913\n144.436\n0.0234436\n\n\nENFJ\n9288\n150.231\n0.0219655\n\n\nISFJ\n8121\n145.577\n0.0192056\n\n\nESTP\n4337\n136.777\n0.0102567\n\n\nESFP\n2215\n128.316\n0.00523833\n\n\nESFJ\n2018\n151.088\n0.00477243\n\n\nESTJ\n1921\n141.99\n0.00454304\n\n\n\n\n\nFollowing the data processing, the MBTI posts analysis was rendered into an interactive visualization using Plotly. This visualization elucidates the distribution of the number of posts and the average length of posts per MBTI type, covering the entire dataset. Bars denote the count of posts, while the line chart overlays the average post length, providing a dual perspective on participation and post detail by type. The graphical representation clearly contrasts the varying levels of engagement among the different personality types and the depth of their interactions. Plotly’s interactivity enriches the user experience, offering a hands-on approach to data discovery and a compelling illustration of the digital footprint of the MBTI community.\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nsummary_df['average_length_of_posts'] = summary_df['average_length_of_posts'].round(2)\nsummary_df['number_of_posts'] = summary_df['number_of_posts']\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add bar chart for number of posts with the specified colors and line properties\nfig.add_trace(\n    go.Bar(\n        x=summary_df['type'],\n        y=summary_df['number_of_posts'],\n        name='Number of Posts',\n        marker=dict(\n            color='rgba(50, 171, 96, 0.6)',\n            line=dict(\n                color='rgba(50, 171, 96, 1.0)',\n                width=1\n            ),\n        ),\n        hovertemplate='%{x}<br>Number of posts: %{y:.f}<extra></extra>'\n    ),\n    secondary_y=False,\n)\n\n# Add line chart for average length of posts with the specified marker color\nfig.add_trace(\n    go.Scatter(\n        x=summary_df['type'],\n        y=summary_df['average_length_of_posts'],\n        name='Average Length of Posts',\n        mode='lines+markers',\n        text=summary_df['average_length_of_posts'],\n        marker=dict(color='rgba(128, 0, 128, 0.9)'),\n        line=dict(color='rgba(128, 0, 128, 0.9)'),  # Set the line color here\n        hovertemplate='%{x}<br>Avg length of posts: %{y:.2f}<extra></extra>'\n    ),\n    secondary_y=True,\n)\n\n# Add figure title\nfig.update_layout(\n    title_text=\"MBTI Posts Analysis: Number of Posts and Average Length\",\n    title_x=0.45,  # This centers the title\n)\nfig.update_layout(\n    title=dict(\n        font=dict(\n            size=20, # Adjusting the font size\n        )\n    )\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"MBTI Type\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"<b>Number of Posts</b>\", secondary_y=False)\nfig.update_yaxes(title_text=\"<b>Average Post Length</b>\", secondary_y=True)\n\n# Show the figure\nfig.show()\n\n\n\n                                                \n\n\nThe interactive chart provides significant insights into the posting behavior among different MBTI personality types on the platform. It reveals that some personality types are more active in posting than others. For instance, types such as INFP and INTJ have a noticeably higher number of posts, indicating that individuals with these personality types may be more inclined to engage in online forums or are more represented in the dataset.\nAdditionally, the visualization shows that while some types like INTJ, INFJ, and INTP have a high volume of posts, the average length of these posts varies, with INTP types tending to write longer posts on average. This could suggest that although INTJs and INFJs are prolific in terms of post numbers, INTPs may delve into more in-depth discussions or provide more detailed contributions to the conversations.\nConversely, types like ESFJ and ESTJ have fewer posts and shorter average post lengths, suggesting these personality types might prefer to engage less frequently and more concisely on the platform or are less represented in the sampled data.\nThese observations are instrumental for understanding how different MBTI personality types participate in online discussions."
  },
  {
    "objectID": "eda.html#posting-behavior-analysis-for-4-mbti-dichotomies-ie-ns-tf-jp",
    "href": "eda.html#posting-behavior-analysis-for-4-mbti-dichotomies-ie-ns-tf-jp",
    "title": "Milestone 1 EDA",
    "section": "4.2 Posting Behavior Analysis for 4 MBTI Dichotomies (I/E, N/S, T/F, J/P)",
    "text": "4.2 Posting Behavior Analysis for 4 MBTI Dichotomies (I/E, N/S, T/F, J/P)\nThe analysis of posting behavior across the 16 distinct MBTI personality types has uncovered intriguing patterns in how different personalities engage in online discourse. Building upon these findings, the focus now shifts to dissecting the four core dimensions that constitute these personality types: Introversion (I) versus Extroversion (E), Intuition (N) versus Sensing (S), Thinking (T) versus Feeling (F), and Judging (J) versus Perceiving (P). By examining these dichotomies, the aim is to distill the essence of how each cognitive preference influences posting frequency and depth. This layered approach will allow for a more granular understanding of the behavioral tendencies that underlie each MBTI axis, providing a richer perspective on the interplay between personality and online interaction patterns.\nTo delve deeper into the nuances of MBTI-related posting behaviors along the fundamental personality dichotomies, the original dataset are filtered and computed the relevant metrics for each MBTI axis.\n\n\nCode\n# Function to calculate the number of posts and average length per MBTI axis\ndef calculate_axis_metrics(posts, axis):\n    # Filter posts containing the specific axis\n    axis_posts = posts[posts['type'].str.contains(axis)]\n    # Calculate the number of posts and the average length\n    number_of_posts = axis_posts.shape[0]\n    average_length_of_posts = axis_posts['post'].str.len().mean()\n    return number_of_posts, average_length_of_posts\n\n# Define the MBTI axes\naxes = ['I', 'E', 'N', 'S', 'T', 'F', 'J', 'P']\n\n# Initialize a dictionary to hold summary data\nsummary_data = {\n    'axis': [],\n    'number_of_posts': [],\n    'average_length_of_posts': [],\n    'percentage_of_posts': []\n}\n\n# Total number of posts\ntotal_posts = posts_expanded.shape[0]\n\n# Calculate the metrics for each axis and store in summary_data\nfor axis in axes:\n    number_of_posts, avg_length = calculate_axis_metrics(posts_expanded, axis)\n    summary_data['axis'].append(axis)\n    summary_data['number_of_posts'].append(number_of_posts)\n    summary_data['average_length_of_posts'].append(avg_length)\n    summary_data['percentage_of_posts'].append((number_of_posts / total_posts) * 100)\n\n# Convert summary_data to a DataFrame\nsummary_df = pd.DataFrame(summary_data)\n\n# Display the summary DataFrame\nsummary_df\n# .to_csv('../data/csv/mbti_post_summary_df_4axes.csv')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\naxis\nnumber_of_posts\naverage_length_of_posts\npercentage_of_posts\n\n\n\n\n0\nI\n325263\n145.785\n76.9225\n\n\n1\nE\n97582\n144.52\n23.0775\n\n\n2\nN\n364822\n146.311\n86.278\n\n\n3\nS\n58023\n140.35\n13.722\n\n\n4\nT\n193533\n143.389\n45.7693\n\n\n5\nF\n229312\n147.269\n54.2307\n\n\n6\nJ\n167110\n147.601\n39.5204\n\n\n7\nP\n255735\n144.116\n60.4796\n\n\n\n\n\n\n\nCode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n# Creating a function to extract values for each dichotomy pair\ndef get_values_for_pair(axis1, axis2, column):\n    values = summary_df[summary_df['axis'].isin([axis1, axis2])][column].values\n    return [round(v, 2) for v in values]\n\n# Colors dictionary\ncolors = {\n    'I': '#A4DEF9',\n    'E': '#3E68BD',\n    'N': '#8DDFB0',\n    'S': '#2C6E49',\n    'T': '#FFF4AB',\n    'F': '#FFB53E',\n    'J': '#E3D8F1',\n    'P': '#A9769E'\n}\n\n# Initialize subplots with shared x-axes and set the spacing here\nfig = make_subplots(\n    rows=2, \n    cols=2, \n    subplot_titles=(\n        'Introversion (I) vs Extroversion (E)',\n        'Intuition (N) vs Sensing (S)',\n        'Thinking (T) vs Feeling (F)',\n        'Judging (J) vs Perceiving (P)'\n    ),\n    specs=[[{\"secondary_y\": True}, {\"secondary_y\": True}],\n           [{\"secondary_y\": True}, {\"secondary_y\": True}]],\n    horizontal_spacing=0.17,  \n    vertical_spacing=0.1     \n)\n\n# Loop through each axis pair and add traces for number of posts only\nfor i, (axis1, axis2) in enumerate([('I', 'E'), ('N', 'S'), ('T', 'F'), ('J', 'P')], start=1):\n    # Calculate row and col for subplot\n    row = (i-1) // 2 + 1\n    col = (i-1) % 2 + 1\n    \n    # Add bar trace for number of posts for axis1\n    fig.add_trace(go.Bar(\n        x=[axis1],\n        y=[get_values_for_pair(axis1, axis2, 'number_of_posts')[0]],\n        name=f'Number of Posts ({axis1})',\n        marker_color=colors[axis1]\n    ), row=row, col=col, secondary_y=False)\n    \n    # Add bar trace for number of posts for axis2\n    fig.add_trace(go.Bar(\n        x=[axis2],\n        y=[get_values_for_pair(axis1, axis2, 'number_of_posts')[1]],\n        name=f'Number of Posts ({axis2})',\n        marker_color=colors[axis2]\n    ), row=row, col=col, secondary_y=False)\n    \n    # The scatter trace for average length of posts is now removed\n\n# Update layout for a cleaner look\nfig.update_layout(\n    title_text='MBTI Dichotomy Comparison: Number of Posts',\n    height=800,\n    showlegend=False,\n    title_x=0.45,\n    barmode='group'\n)\nfig.update_layout(\n    title=dict(\n        font=dict(\n            size=20,\n        )\n    )\n)\n\n# Update y-axes titles for primary y-axes (number of posts)\nfig.update_yaxes(title_text='Number of Posts', secondary_y=False)\n\n# Show figure\nfig.show()\n\n\n\n                                                \n\n\nThe visualization provides a comparative analysis of the MBTI dichotomies, offering valuable insights into the posting behaviors associated with each of the four dimensions.\n\nIntroversion (I) vs Extroversion (E): There is a notable prevalence of posts from introverted types, as indicated by the significantly higher number of posts under ‘I’ compared to ‘E’. This suggests that introverted users are more active in posting on the platform or that they represent a larger portion of the posting population. Additionally, the average post length for ‘I’ is marginally higher than for ‘E’, implying that introverts might engage in more detailed or elaborate discussions.\nIntuition (N) vs Sensing (S): The ‘N’ type dominates in both the number of posts and the average post length compared to ‘S’. This could indicate that intuitive individuals are not only more represented but also tend to share more comprehensive posts, perhaps reflecting a preference for abstract or theoretical discussions typical of intuitive personality types.\nThinking (T) vs Feeling (F): The distribution here is quite balanced in terms of post count, with a slight edge to ‘F’ types. However, ‘F’ types appear to write longer posts on average than ‘T’ types, which could suggest that those with a feeling preference are more inclined to elaborate on their sentiments and values, or they engage in more narrative or expressive discussions.\nJudging (J) vs Perceiving (P): The ‘P’ preference exhibits a higher number of posts and a slightly longer average post length than the ‘J’ preference. This might reflect a tendency for perceiving types to participate more frequently and with a bit more elaboration in online forums, possibly due to their more spontaneous and adaptable nature."
  },
  {
    "objectID": "eda.html#correlation-between-mbti-personality-types-and-postural-health",
    "href": "eda.html#correlation-between-mbti-personality-types-and-postural-health",
    "title": "Milestone 1 EDA",
    "section": "5.1 Correlation Between MBTI Personality Types and Postural Health",
    "text": "5.1 Correlation Between MBTI Personality Types and Postural Health\nThis EDA forms part of a broader study aimed at discerning patterns between MBTI personality types and postural health. The EDA seeks to uncover whether certain postural categories, such as ‘Flat back,’ ‘Ideal posture,’ ‘Kyphosis-lordosis,’ and ‘Sway-back,’ are more prevalent within specific MBTI classifications. This investigation is structured to provide insights that could be pivotal in developing targeted health programs. By aggregating individual pain indicators into a comprehensive measure and categorizing postural data, the analysis facilitates a nuanced understanding of how postural health might correlate with personality type.\n\n\nCode\n#mbti_health_df = pd.read_csv(\"mbti_health.csv\")\nmd = tabulate(mbti_health_df.head(6), headers='keys', tablefmt='pipe',showindex=False)\nd.Markdown(md)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS No\nAGE\nHEIGHT\nWEIGHT\nSEX\nACTIVITY LEVEL\nPAIN 1\nPAIN 2\nPAIN 3\nPAIN 4\nMBTI\nE\nI\nS\nN\nT\nF\nJ\nP\nPOSTURE\nSUM PAIN\nPOSTURE DETAIL\n\n\n\n\n1\n53\n62\n125\nFemale\nLow\n0\n0\n0\n0\nESFJ\n18\n3\n17\n9\n9\n13\n18\n4\nA\n0\nIdeal posture\n\n\n2\n52\n69\n157\nMale\nHigh\n7\n8\n5\n3\nISTJ\n6\n15\n14\n12\n21\n3\n13\n9\nB\n23\nKyphosis-lordosis\n\n\n3\n30\n69\n200\nMale\nHigh\n0\n0\n0\n0\nESTJ\n15\n6\n16\n10\n15\n9\n12\n10\nA\n0\nIdeal posture\n\n\n4\n51\n66\n175\nMale\nModerate\n9.5\n9.5\n9.5\n1.5\nISTJ\n6\n15\n21\n5\n13\n11\n19\n3\nD\n30\nSway-back\n\n\n5\n45\n63\n199\nFemale\nModerate\n4\n5\n2\n2\nENFJ\n14\n7\n20\n6\n9\n15\n16\n6\nA\n13\nIdeal posture\n\n\n6\n68\n74\n182\nMale\nLow\n0\n2.5\n1.5\n0\nISFP\n4\n17\n17\n9\n11\n13\n4\n18\nD\n4\nSway-back\n\n\n\n\n\n\n\nCode\n# Importing necessary packages\nimport plotly.figure_factory as ff\nimport pandas as pd\n\n# Creating a crosstab to get the frequency of each MBTI type for each posture category\ncross_tab = pd.crosstab(mbti_health_df['MBTI'], mbti_health_df['POSTURE DETAIL'])\n\n# Calculating the row-wise sum to use in percentage calculation\nrow_sum = cross_tab.sum(axis=1)\n\n# Using broadcasting to calculate the percentage\ncross_tab_percentage = cross_tab.div(row_sum, axis=0)\n\n# Creating a heatmap using plotly\n\nfig = ff.create_annotated_heatmap(z=cross_tab_percentage.values,\n                x=cross_tab_percentage.columns.tolist(),\n                y=cross_tab_percentage.index.tolist(),\n                annotation_text=cross_tab_percentage.applymap(\"{:.2f}\".format).values,\n                colorscale='YlGnBu',\n                )\n\n# Customize hover information\nfig.update_traces(\n    hovertemplate=\"<b>MBTI Type:</b> %{y}<br>\" +\n                  \"<b>Posture Category:</b> %{x}<br>\" +\n                  \"<b>Percentage:</b> %{z:.2f}<extra></extra>\"  # Hide the trace name\n)\n\nfig.update_layout(\n    title=dict(\n        text='Heatmap of MBTI Types and Posture Categories (Percentage)',\n        x=0.5,  # This will center the title\n        xanchor='center',\n        font=dict(\n            size=19, # Adjusting the font size\n        )\n    ),\n    xaxis=dict(title='Posture Categories', side='bottom'),  \n    yaxis=dict(title='MBTI Types'),\n    coloraxis_colorbar=dict(\n        title='Percentage',  \n        x=1, \n        xanchor='left', \n    ),  \n    autosize=False,\n    width=855,\n    height=679\n)\n\n# Displaying the plot\nfig.show()\n\n\n\n                                                \n\n\nThis heat map shows the association between different MBTI personality types and different postural problems. The shades of color in the heat map represent percentages, with darker colors representing higher percentages and lighter colors representing lower percentages.\nEach MBTI type corresponds to four posture problems: Flat back, Ideal posture, Kyphosis-lordosis, and Sway-back.\nFrom this chart, we can see:\n\nAmong all the people who took this fitness test, ENFP individuals had the highest proportion in the “ideal posture” column, which means that compared to other MBTI types, ENFPs are more likely to have good posture.\nThe proportion of ISFJ types in the “Flat Back” column is higher, which may indicate that ISFJ individuals may need to pay attention to improving the back curve in daily life to avoid problems caused by flat backs.\nThe proportion of individuals of ISFP, INTJ and INFP types in the “hunchback-lumbar protrusion” column is relatively low, which indicates that individuals of these three types are less likely to have such postural problems.\nISFP, ISTJ, INFP, and INFJ individuals have a higher proportion in the “forward leaning” column, which may mean that these four types of individuals need to pay attention to the position of the spine and pelvis and avoid long-term forward leaning postures."
  },
  {
    "objectID": "eda.html#age-related-pain-trends-across-mbti-personality-types",
    "href": "eda.html#age-related-pain-trends-across-mbti-personality-types",
    "title": "Milestone 1 EDA",
    "section": "5.2 Age-Related Pain Trends Across MBTI Personality Types",
    "text": "5.2 Age-Related Pain Trends Across MBTI Personality Types\nThis scatter plot is an exploratory tool designed to delve into the interplay between age, pain levels, and MBTI personality types. By plotting the average age against reported pain intensities and using bubble sizes to represent the prevalence of each MBTI type, the visualization aims to surface any apparent trends or anomalies. The analysis is structured to facilitate the identification of age-related pain patterns within different personality types, providing a foundation for subsequent statistical examination and the potential development of personalized health interventions. This initial graphical representation sets the stage for a deeper investigation into how personality traits might influence the experience and reporting of pain across age groups.\n\n\nCode\n# Create the scatter plot\nimport plotly.express as px\ncolor_list=['#ffffd9', '#f5fbc4', '#eaf7b1', '#d6efb3', '#bde5b5', '#97d6b9', '#73c8bd', '#52bcc2', '#37acc3', '#2498c1', '#1f80b8', '#2165ab', '#234da0', '#253795', '#172978', '#081d58']\n\n# Group the data by 'MBTI' and calculate the average count for each MBTI type\nmbti_grouped = mbti_health_df.groupby('MBTI').agg(\n    Avg_age=('AGE', 'mean'),\n    Avg_pain=('SUM PAIN', 'mean'),\n    Count=('MBTI', 'size')\n).reset_index()\n\nmbti_grouped['Avg_age'] = mbti_grouped['Avg_age'].round(2)\nmbti_grouped['Avg_pain'] = mbti_grouped['Avg_pain'].round(2)\n\nfig = px.scatter(mbti_grouped, y='Avg_pain', x='Avg_age', \n                 size='Count', color='MBTI', \n                 hover_name='MBTI', size_max=60,color_discrete_sequence=color_list)\n\n# Hide the legend\nfig.update_layout(showlegend=True)\n\n# Add MBTI type labels for each point on the scatterplot\nfor index, row in mbti_grouped.iterrows():\n    fig.add_annotation(\n        y=row['Avg_pain'],\n        x=row['Avg_age'],\n        text=row['MBTI'],\n        showarrow=False,\n        yshift=12\n    )\n\n\nfig.update_layout(title=dict(\n        text='MBTI Types: Average Age vs PAIN with Sequency',\n        x=0.5,  # This will center the title\n        xanchor='center',\n        font=dict(\n            size=19, # Adjusting the font size\n        )\n    ),\n    xaxis=dict(title='Average Age'),  \n    yaxis=dict(title='Average Pain'),\n    )\n\n# Show the figure\nfig.show()\n\n\n\n                                                \n\n\nThis bubble scatter plot shows the relationship between MBTI personality type, average age, and average pain level. The size of the bubble represents the frequency of that mbti.\nFrom this chart, we can observe:\n\nTypes like ISTP and ESTJ show higher pain levels at older ages, possibly because this hints at underlying age-related health trends and personality type associations.\nINFJ and ENFJ show very high pain levels in the middle age group. This may be because people with the three characteristics of NF are more planned and are intuitive feeling individuals. They are so serious about their work that they sit down at their desk until the task is completed, so they often suffer from pain caused by incorrect posture.\n\nThe analysis of these two EDA charts provides data support for formulating targeted health strategies and helps us better understand the connection between MBTI personality types and health problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Our story begins with a simple fact: we four are the best of friends. From the first day of our program, we gravitate towards one another, becoming not just friends but also classmates and teammates on numerous projects. As time passed, we discovered that despite our diverse hobbies, habits, and personalities, we remained deeply connected. One day, we shared our MBTI types and were fascinated to find we each had a distinct one! This revelation led us to attribute our differences to our MBTI types, attempting to understand one another’s behaviors through this lens. It was intriguing—like how, our INFP friend, who could capture the beauty of a single flower in a thousand photographs, while the ESTJ friend would hardly notice the changing weather. However, we realized that hastily pigeonholing our behaviors under MBTI seemed too simplistic, and thus we turned to data science to dig deeper. This curiosity is the genesis of our project.\nThe MBTI personality test categorizes individuals into one of 16 personality types based on preferences in four dimensions: Extraversion (E) vs. Introversion (I), Sensing (S) vs. Intuition (N), Thinking (T) vs. Feeling (F), and Judging (J) vs. Perceiving (P). Each letter represents a trait, with each combination forming a personality type that provides a fun and insightful reflection of how we interact with the world. To enrich your understanding and involvement in this project, we invite you to take the test at 16Personalities.\nIn this project, we conduct a comprehensive analysis of submissions and comments on the MBTI subreddit. We aim to uncover the distinct behaviors of the various MBTI types online—ranging from the number of posts and discussed topics to the length of responses and the popularity of related discussions. We also integrate data beyond textual information, most notably health data. While MBTI tests primarily focus on personality types and psychological preferences, they seldom link to physical health. Our goal is to offer viewers new perspectives to understand their own and others’ MBTI types more holistically.\nUnderstanding MBTI types matters because it offers a framework for appreciating our differences and navigating interpersonal dynamics. This project aims to deepen that understanding by correlating MBTI types with online behavior and health data, potentially unveiling new insights into our digital and physical well-being. By engaging with this analysis, we hope to foster a greater appreciation for the diversity of personalities and their unique contributions to our world."
  },
  {
    "objectID": "index.html#eda-topics",
    "href": "index.html#eda-topics",
    "title": "Introduction",
    "section": "EDA Topics",
    "text": "EDA Topics\n\nBusiness Goal: Gain insights into the engagement levels within the MBTI subreddit community by analyzing the volume and intensity of discussions over the last two years. Identify the most captivating topics and explore the depth of interactions through the length of responses.\nTechnical Proposal:\n\nQuery the dataset to calculate the total number of posts over the past two years.Break down the post count yearly to discern any trends in volume.\nApply sorting algorithms to organize posts by the number of comments received. Isolate and detail the top 10 titles that garnered the most community interaction.\nIntroduce a “word_count” column derived from splitting titles and comments by spaces and tallying the resulting words.\nConstruct and analyze the distribution of word counts for both submissions and comments to determine the general length and depth of community responses.\n\nBusiness Goal: Determine which of the 16 personality types garners the most discussion topics (submissions) and which type elicits the most engagement through replies (comments). Also, find out the main content of the most replied-to post for each personality type.\nTechnical Proposal: For each submission title, employ pattern matching to identify mentions of any MBTI personality types (e.g., ESTJ, ISTJ, ENFP, etc.). Create a new column, “mbti_type_related”, to label the relevant personality type associated with the post. If no specific type is discerned in the title, label it as “general” under the “mbti_type_related” column. Then, group by the “mbti_type_related” column and:\n\nCount the total number of submissions per type to determine the most frequently discussed personality type.\nSum up the comments to find out which personality type receives the most engagement through replies.\nSort by the number of comments in descending order to identify the most engaged post for each personality type, capturing its core content or theme for reference.\n\nBusiness Goal: Track and visualize the evolution of discussions around MBTI to understand changing trends, peaks of interest, and any cyclic patterns over the data coverage period.\nTechnical Proposal:\n\nQuery the dataset to summarize the total number of posts and break down the post count, comment count, and track the average score monthly to understand the trends.\nVisualize the changing monthly trends and apply time-series analysis to detect recurring patterns and notable deviations of MBTI discussions over the past two years.\nSummarize key trends and patterns with a focus on significant peaks and troughs.\n\nBusiness Goal: Understand the posting behavior and distribution across different MBTI personality types by analyzing the volume and average length of posts for each type as well as along the four MBTI dichotomies: Introversion/Extroversion, Intuition/Sensing, Thinking/Feeling, Judging/Perceiving.\nTechnical Proposal:\n\nIntroduce an external dataset containing posts and the user’s MBTI types.\nParse the dataset to split posts, categorize them by MBTI type, and generate a summary table detailing the total posts and average post length for each of the 16 MBTI personality types.\nConsolidates the data based on the four MBTI dichotomies (I/E, N/S, T/F, J/P), showing the number of posts and average post length for each trait.\nVisualize the data to extract insights on posting patterns, such as which MBTI types are more active or which types tend to write longer posts.\nDetermine if certain dichotomies correlate with more frequent or more verbose posting behaviors.\nSummarize the findings in a comprehensive report that details the distinctive posting features of the MBTI types and dichotomies.\n\nBusiness Goal: Identify patterns in health-related data that correlate with MBTI personality types. The analysis is designed to identify trends in postural categories and pain levels across age groups to inform targeted health programs and interventions.\nTechnical Proposal: Individual pain indicators for each MBTI personality type were combined into a cumulative “SUM PAIN” measure to analyze overall pain trends. Postural data was divided into different categories, revealing posture-related health conditions in different MBTI signatures. The data were then grouped by MBTI type, and mean age and pain scores and type frequencies were calculated to identify important health patterns across personality groups.\n\nConduct a statistical analysis to explore the relationship between MBTI types and posture-related health outcomes, utilizing a heatmap to visualize the prevalence of certain postures across the MBTI spectrum.\nEmploy a scatter plot analysis to examine the correlation between age, the frequency of reported pain, and MBTI types, highlighting the average pain levels with bubble sizes to indicate the frequency of each type."
  },
  {
    "objectID": "index.html#nlp-topics",
    "href": "index.html#nlp-topics",
    "title": "Introduction",
    "section": "NLP Topics",
    "text": "NLP Topics\n\nBusiness Goal: This proposal aims to explore sentiment trends in relation to comment scores within the MBTI subreddit community. Our goal is to determine if higher-scoring comments correlate with more positive sentiments. This analysis is intended to provide insights into user engagement and the emotional content of highly-rated comments.\nTechnical Proposal:\n\nWe will first categorize the original ‘comment_score’ into four levels (‘Low’, ‘Medium’, ‘High’, ‘Very High’) using the 25th, 50th, and 75th percentiles.\nThen we will apply a pretrained sentiment analysis model to classify each comment as ‘positive’, ‘neutral’, or ‘negative’.\nBy grouping these results according to our score categories and visualizing the data with a heatmap, we aim to reveal any significant patterns or correlations between the comment scores and their respective sentiments.\n\nBusiness goal: Through an analysis of numerous conversations on Reddit, certain topics emerge as the most prevalent. Our goal is to comprehend the predominant subjects within MBTI discussions.\nTechnical Proposal:\n\nImplement Wordcloud to see the common words in Reddit submission titles.\nUse TF-IDF to get the important words in each Reddit submission.\nData collection and preparation: Filtering the discussions related to the MBTI discussion. Then conduct data preprocessing steps for text data: tokenization, remove stop words, and Count Vectorization.\nApply NLP topic modeling techniques (LDA) on submission to identify prevalent discussion topics and also get the weight of each topic word to see the dominant words in each topic.\n\nBusiness Goal: Analyze linguistic patterns and topic preferences within the MBTI community by examining the diversity of language used in posts and identifying topics or keywords that resonate with each of the 16 MBTI personality types and the four dichotomous axes (I/E, N/S, T/F, J/P).\nTechnical Proposal:\n\nCalculate metrics like Lexical Density, Lexical Variety, and Average Word Length for each post. Analyze the use of unique words and complexity of language for each MBTI type to assess the diversity in vocabulary, syntax, and readability among the posts of different MBTI types.\nUse frequency analysis to determine the most common words and phrases for each MBTI type and across the dichotomous axes.\nDevelop visual representations, such as word clouds, to illustrate the unique language use and topic interests of each MBTI type and axis."
  },
  {
    "objectID": "index.html#ml-topics",
    "href": "index.html#ml-topics",
    "title": "Introduction",
    "section": "ML Topics",
    "text": "ML Topics\n\nBusiness goal: Employ the MBTI Health Dataset for fitting a linear regression model that predicts an individual’s total pain range, which spans from 0 to 40. Our goal is to explore how various factors, including posture, impact an individual’s pain level and to ascertain if specific MBTI characteristics significantly influence this pain level.\nTechnical Proposal:\n\nData preprocessing including encoding, define pipeline, correlation exploration, capture the intricate and non-linear relationships within the posture data,\nIncorporate a variety of explanatory variables: physical attributes (height, weight, activity level), basic demographic information (age, sex), and personality traits as indicated by the Myers-Briggs Type Indicator (MBTI).\nBuild linear regression model, assess model efficacy through two pivotal metrics: the Root Mean Squared Error (RMSE).\nSave the model for future use and apply model to new data\n\nBusiness goal: By analyzing MBTI-related discussions on Reddit, our aim is to identify the degree of association between different MBTI personality types. This analysis seeks to understand the interconnectedness of different personality types in online communication and expression. The insights gained will enhance our understanding of how various personality types interact in the digital space.\nTechnical Proposal:\n\nText Preprocessing: Clean, lexically annotate, and recognize entities in MBTI-related posts from Reddit to prepare the text data for analysis.\nUsing the FP-Growth Model: Explore the text data using the FP-Growth algorithm to find frequent item sets and association rules between different MBTI types.\nAnalysis of Frequent Item Sets and Association Rules: Calculate the support and confidence of the association rules to ensure the reliability and significance of the identified patterns. Perform a descending-order sorting of the frequent item sets and association rules to highlight the most significant patterns.\nDrawing and Displaying Association Rule Network Graphs: For the identified association rules, create network graphs to visually represent the relationships between different MBTI types.\n\nBusiness goal: Develop a predictive model that can identify an individual’s MBTI personality type based on their online text entries.\nTechnical Proposal:\n\nClean the text data to remove irrelevant information, such as special characters, URLs, and non-standard language elements.\nApply natural language processing techniques to tokenize, and remove stopwords from the text data for further analysis.\nTrain machine learning models (e.g., logistic regression, support vector machines, random forests to predict MBTI types based on the engineered features.\nDefine clear metrics to compare model performances, focusing on accuracy, F1 score, Precision, and Recall to measure the model’s ability to classify the posts into different MBTI types.\nPlot confusion matrices to analyze the model’s performance in predicting each MBTI type."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Project summary\n\n1.MBTI Mirror: Community Engagement Disposition Mapping (EDA)\nThrough in-depth exploratory data analysis of post and comment data from the MBTI subreddit community, we discovered significant engagement changes and community activity trends. The analysis shows engagement patterns across different MBTI types, revealing that certain types of posts and comments are more frequent than others. This finding provides valuable insight into the distribution and interactions of personality types within communities.\n\n\nCode\n# load the preprocessed data\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport kaleido\n\ndf = pd.read_csv(\"../data/csv/time-series.csv\")\n\n# Create subplots\nfig = make_subplots(rows=2, cols=1, subplot_titles=(\"Monthly Submission Counts\", \"Average Comments and Scores\"))\n\n# Line plot \nfig.add_trace(\n    go.Scatter(x=df['submission_year'].astype(str) + '-' + df['submission_month'].astype(str), \n               y=df['count(submission_title)'], \n               name='Submission Counts', \n               mode='lines+markers'),\n    row=1, col=1\n)\n\n# Bar plot\nfig.add_trace(\n    go.Bar(x=df['submission_year'].astype(str) + '-' + df['submission_month'].astype(str), \n           y=df['avg_num_comments'], \n           name='Average Number of Comments'),\n    row=2, col=1\n)\n# Scatter plot \nfig.add_trace(\n    go.Scatter(x=df['submission_year'].astype(str) + '-' + df['submission_month'].astype(str), \n               y=df['avg_score'], \n               name='Average Score', \n               mode='markers'),\n    row=2, col=1\n)\n\n# Update xaxis\nfig.update_xaxes(title_text=\"Date\", row=1, col=1)\nfig.update_xaxes(title_text=\"Date\", row=2, col=1)\n\n# Update yaxis \nfig.update_yaxes(title_text=\"Submission Counts\", row=1, col=1)\nfig.update_yaxes(title_text=\"Average Values\", row=2, col=1)\n\n# Update title and layout\nfig.update_layout(\n    title_text=\"Reddit MBTI Topic Submission Trends\",\n    title_x=0.45,  # This centers the title\n    showlegend=True,\n\n)\nfig.update_layout(\n    title=dict(\n        font=dict(\n            size=20, # Adjusting the font size\n        )\n    )\n)\n\n# Show the figure\nfig.show()\n\n\n\n                                                \n\n\n\n\n2.MBTI Context: Exploring the discourse world of introversion and extroversion (NLP)\nUsing NLP technology to analyze the discussion content in the community, we successfully identified the main discussion topics and key words about introversion and extroversion. This not only reveals common perceptions of these character traits among community members, but also demonstrates differences in emotional tendencies and word choice. These insights provide a deeper understanding of the nuances of language expression across MBTI types.\n   \n\n\n3.MBTI Decoded: The Power of Predicting Personality with Machine Learning (ML)\nBy building and applying machine learning models, we successfully predicted MBTI types, demonstrating the model’s remarkable performance in accuracy and performance. Additionally, our analysis explores potential links between MBTI types and health data, revealing specific health risks and patterns that different personality types may face. These insights are extremely important for designing targeted health intervention strategies.\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objs as go\nimport networkx as nx\n\nrules_pd=pd.read_csv(\"../data/csv/ordered_association_rules.csv\")\n\n# Create a graph\nG = nx.DiGraph()\n\n# Add nodes and edges\nfor _, row in rules_pd.iterrows():\n    G.add_edge(str(row['antecedent']), str(row['consequent']), weight=row['confidence'])\n\n# Generate position layout\npos = nx.spring_layout(G)\n\n# Create edge trace\nedge_x = []\nedge_y = []\nfor edge in G.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.extend([x0, x1, None])\n    edge_y.extend([y0, y1, None])\n\nedge_trace = go.Scatter(\n    x=edge_x, y=edge_y,\n    line=dict(width=0.5, color='#888'),\n    hoverinfo='none',\n    mode='lines')\n\n# Create node trace\nnode_x = []\nnode_y = []\nfor node in G.nodes():\n    x, y = pos[node]\n    node_x.append(x)\n    node_y.append(y)\n\n#color_list=['#ffffd9', '#f5fbc4', '#eaf7b1', '#d6efb3', '#bde5b5', '#97d6b9', '#73c8bd', '#52bcc2', '#37acc3', '#2498c1', '#1f80b8', '#2165ab', '#234da0', '#253795', '#172978', '#081d58']\ncolor_list=['#081d58','#253795','#1f80b8','#97d6b9','#ffffd9']\n\nnode_trace = go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers+text',  # Add 'text' to the mode\n    text=[node for node in G.nodes()],  # Add node labels\n    textposition=\"bottom center\",  # Position of text\n    hoverinfo='text',\n    marker=dict(\n        showscale=True,\n        colorscale=color_list,\n        reversescale=True,\n        color=[],\n        size=15,  # Increase node size\n        colorbar=dict(\n            thickness=15,\n            title='Number of Node Connections',\n            xanchor='left',\n            titleside='right'\n        ),\n        line_width=1.5))\n\n# Add node text and hover info\nnode_adjacencies = []\nnode_text = []\nfor node, adjacencies in enumerate(G.adjacency()):\n    node_adjacencies.append(len(adjacencies[1]))\n    node_text.append(f'{adjacencies[0]}')\n\nnode_trace.marker.color = node_adjacencies\nnode_trace.text = node_text\n\n# Create figure\nfig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title='<br>Network graph of association rules',\n                titlefont_size=23,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=80),\n                annotations=[dict(\n                    text=\"Python plotly library\",\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002)],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\n\nfig.show()\n\n\n\n                                                \n\n\nThrough an in-depth analysis of the MBTI subreddit community, this project successfully achieved insights into community engagement, MBTI type-specific discussions, trends over time, posting behavior within different personality dyads, and correlations with health data. Comprehensive understanding. This project also sheds light on how personality types shape patterns of communication and behavior in a community, providing a unique insight into this complex and diverse community.\n\n\n\nNext Steps Plan\nTo further deepen our understanding of the MBTI subreddit community and improve the comprehensiveness of our analysis, our future plans will include several key directions.\n\nFirst, we plan to examine comments and posts that may have been deleted or removed to ensure our data set is as complete as possible to provide a more accurate analysis of community engagement.\nSecond, we will set out to collect more MBTI-related health data to make broader generalizations about the relationship between different personality types and health conditions. This will not only enhance our current health-related analyses, but may also reveal new interesting associations.\nFinally, we plan to increase interaction with users and audiences. By participating more actively in the community, we can better understand the needs and interests of our users, thereby further improving our analysis methods and research directions.\n\n\n\nWhat we learned from this project\n\n During the exploration of this project, I found that INFP people like me like to explore topics about MBTI on Reddit. Based on the actual situation of my daily use of the Internet, it seems that this is indeed the case. I pay attention to exploring people’s inner voices. I like to listen to people telling their own stories online, and I am also very empathetic. In the exploration of MBTI and physical health, it is true that due to my personality, I like to stay at home and do not like to exercise. And this also caused my spinal health to be in a bad situation. This analysis reinforced my need to change my habits to improve my health. \n\n\n In our MBTI project, I’ve come to a somewhat bittersweet realization about my own personality type: we’re the unsung heroes of the MBTI world, rarely the center of online discussions. Our analysis showed that ESTJs, like me, are less inclined to share our lives on the internet, preferring real-life interactions. And a striking personal revelation came from our health data analysis - it turns out ESTJs are prone to posture-related issues. As a desk-bound workaholic, I could relate all too well, having experienced the exact ailments our data pointed out. \n\n\nAs expected, individuals with F/P personality types tend to share their emotions on social media more than those with T/J types. Surprisingly, contrary to common perceptions, I found that I (introverted) personalities posted over three times more than E (extroverted) ones. This could indicate that introverts, often perceived as reserved, might find social media a comfortable platform to express themselves, more so than extroverts. Additionally, there’s a noticeable trend in MBTI discussions on social media, with peaks at the start and end of each year. This suggests that people are more inclined to explore and discuss their MBTI types during periods of self-reflection and while setting New Year’s resolutions.\n\n  Through this project, I’ve observed that individuals with an extraverted (E) personality type tend to be active on social media, and their posts are easily discoverable. On the other hand, those with an introverted (I) personality type seem to be more reserved, often avoiding the sharing of their thoughts on social platforms. Consequently, I posit that individuals with an introverted personality type are more likely to exude a sense of mystery. Additionally, I’ve come across an intriguing observation regarding the correlation between MBTI types and individuals’ health situations. This connection might be attributed to their habits and lifestyle choices."
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "Feedback Discussion",
    "section": "",
    "text": "EDA\n\n1. The distribution of content length should have been calculated on the entire dataset since no individual metrics are required, just percentiles (from Professor Anderson)\n\nThe initial intent behind employing a data sample was to illustrate interactive plots with Plotly. However, when attempting to generate a plot for the complete dataset using Plotly, errors occurred, making it challenging to display the plots even with PySpark. In response to feedback, we now consider showcasing the entire dataset to be more crucial than displaying only a portion of the data with Plotly. To address this issue, we have modified the original plot to one that encompasses the entire dataset using Matplotlib instead.\n\n\n\n2. Why does Topic 2 have only a very small number of posts? (from Professor Anderson)\n\nIn Topic 2, our analysis focuses exclusively on posts that mention only one MBTI type, identified through regex patterns in submission titles. The limited number of posts in this topic results from many titles either not specifying an MBTI type or mentioning multiple types. Posts with more than one MBTI type are reserved for future analysis exploring associations between different MBTI types.\n\n\n\n3. Rounding for plotly labels in Topic 4. Topic 4 dichotomy charts all have different y axes, which makes things confusing to interpret. The dual axis and red line makes it seem like there is a substantial difference between post lengths but the difference is only a few characters. (from Professor Anderson)\n\nWe’ve refined the Plotly labels in Topic 4 to display only two decimal places for greater clarity. Acknowledging your feedback, we realize the dual-axis approach in our dichotomy charts may have inadvertently caused confusion, particularly regarding the minor differences in post lengths. To address this, we will remove the red line and the right-axis related to average post length, focusing solely on the number of posts difference between MBTI dichotomies for clearer interpretation.\n\n\n\n4. The number of entries for topic 5 health data is very low. (from Professor Anderson)\n\nThe scarcity of entries for Topic 5’s health data is due to the limited availability of open-source MBTI-related health information on the internet. While MBTI primarily focuses on personality types and psychological preferences, there’s minimal analysis linking it to physical health and behavior. Despite the data constraints, we pursued this analysis to uncover unique insights and narratives related to MBTI and health.\n\n\n\n5. Notably, the table generated from the PersonalityPost external dataset contains numerous links. It would enhance the cleanliness and readiness of the data for further analysis to clean these links. (from DSAN 6000 Peers)\n\nThis is a good catch! We weren’t initially aware of the extensive presence of links in the PersonalityPost dataset. To address this, we’ve now cleaned the data by removing these links using regex patterns, enhancing its clarity and readiness for further analysis.\n\n\n\n6. It would be beneficial to check for comments and submissions that may have been removed or deleted, as well as for duplicate and identical responses. Additionally, assessing the presence of any completely null or irrelevant (flooded) replies is important, especially since they might still be counted in your analysis. (from DSAN 6000 Peers)\n\nWe acknowledge the importance of removing empty, deleted, or duplicate responses for more accurate analysis. However, due to our NLP and ML sections being finalized with the existing dataset, revising it now would invalidate our completed work, and constraints in time and budget prevent us from redoing the analysis for this Milestone. If time and resources permit in the future, we definitely plan to revisit this project with a refined dataset.\n\n\n\n\nNLP work\n\n1. Heatmap of sentiment is good. What are the percentages though? Hard to tell if positive is more likely to be a low score or vice versa. (From Professor Marck)\n\nWe recognize the imbalance in the total number of sentiments in our heatmap. To provide a clearer comparison, we’ve now included a percentage chart alongside the heatmap, ensuring a more balanced and fair representation of sentiment labels across different groups.\n\n\n\n2. Sampling to show the word length distribution isn’t great. Fix and do in pyspark. We’ve talked about bad sampling issues. (From Professor Marck)\n\nAfter receiving the feedback, we noticed that the sampling employed was insufficient for illustrating the overall word length distribution. In response, we have opted to switch the package used. Specifically, we utilize Matplotlib for the comment length distribution and Plotly for the submission title. Our final distribution plot incorporates the entire dataset through PySpark.\n\n\n\n\nWebsite/results\n\n1. I noticed that the writing, although professional and grammatically sound, could benefit from a more engaging style. Incorporating analysis related to your own MBTI characteristics might add a personal touch and make the content more relatable to the audience. (From DSAN 6000 Peers)\n\nFantastic suggestion! To add a personal and engaging touch, our conclusion will feature a discussion about each of our unique MBTI types and the insights we’ve gained about ourselves and each other through this project. Interestingly, all four of us represent different MBTI types, which should make for a diverse and intriguing perspective!\n\n\n\n2. To enhance understanding, consider providing more background information on the 16 personality types outlined in your business goals. (From DSAN 6000 Peers)\n\nWe’ve included a detailed introduction to the 16 MBTI personality types at the start of our project to establish a solid foundation for our audience. Additionally, we’ve provided a link to an MBTI personality test, allowing users unfamiliar with their type to discover it before delving into our project.\n\n\n\n3. Link to your EDA notebook is not accessible to the public. Use a private browser to confirm public access. (From DSAN 6000 Peers)\n\nWe’ve updated the access settings on our GitHub repository to ensure public accessibility. Now, by clicking the link provided at the beginning of each milestone, users can easily access all our work, including code and outputs."
  },
  {
    "objectID": "ml.html#analysis-goal",
    "href": "ml.html#analysis-goal",
    "title": "Milestone 3 ML",
    "section": "1.1 Analysis Goal",
    "text": "1.1 Analysis Goal\nIn this analysis, our objective is to employ the MBTI Health Dataset for fitting a linear regression model that predicts an individual’s total pain range, which spans from 0 to 40. This model will incorporate a variety of explanatory variables: physical attributes (height, weight, activity level), basic demographic information (age, sex), and personality traits as indicated by the Myers-Briggs Type Indicator (MBTI). The MBTI results are expressed through a combination of dichotomous traits—Extraversion (E) vs. Introversion (I), Sensing (S) vs. Intuition (N), Thinking (T) vs. Feeling (F), and Judging (J) vs. Perceiving (P)—each assigned a level between 0 and 26. Due to the complementary nature of these traits (e.g., a score of 16 in E implies a score of 10 in I), to prevent multicollinearity, we will include only one trait from each dichotomy in our model, specifically E, S, T, and J. Our goal is to explore how various factors, including posture, impact an individual’s pain level and to ascertain if specific MBTI characteristics significantly influence this pain level.\nLink to Regression Analysis Notebook Code"
  },
  {
    "objectID": "ml.html#data-preprocessing",
    "href": "ml.html#data-preprocessing",
    "title": "Milestone 3 ML",
    "section": "1.2 Data Preprocessing",
    "text": "1.2 Data Preprocessing\n\n1.2.1 Create Total Pain Variable\nIn the initial phase of our data preprocessing, we computed a ‘total pain’ variable by summing four discrete pain measurements—neck, thoracic, lumbar, and sacral areas—each ranging from 0 to 10. This integrated variable therefore extends from 0 to 40. To streamline our dataset and avert multicollinearity, we subsequently removed the original quartet of pain variables.\n\n\n\nData with Total_pain \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGE\nHEIGHT\nWEIGHT\nSEX\nACTIVITY LEVEL\nE\nI\nS\nN\nT\nF\nJ\nP\nPOSTURE\ntotal_pain\n\n\n\n\n53\n62\n125\nFemale\nLow\n18\n3\n17\n9\n9\n13\n18\n4\nA\n0\n\n\n52\n69\n157\nMale\nHigh\n6\n15\n14\n12\n21\n3\n13\n9\nB\n23\n\n\n30\n69\n200\nMale\nHigh\n15\n6\n16\n10\n15\n9\n12\n10\nA\n0\n\n\n51\n66\n175\nMale\nModerate\n6\n15\n21\n5\n13\n11\n19\n3\nD\n30\n\n\n45\n63\n199\nFemale\nModerate\n14\n7\n20\n6\n9\n15\n16\n6\nA\n13\n\n\n\n\n\n\n\n1.2.2 Correlation Matrix\nSubsequently, we performed a correlation analysis on the numerical variables, namely ‘AGE’, ‘HEIGHT’, ‘WEIGHT’, ‘E’, ‘I’, ‘S’, ‘N’, ‘T’, ‘F’, ‘J’, ‘P’, and ‘total_pain’, visualizing the interrelations through a heatmap. This matrix shows the degree of correlation, which we aimed to keep minimal among independent variables, while anticipating a robust linkage with the dependent variable. The heatmap confirms our premise; the MBTI dichotomous traits exhibit significant intercorrelations, validating our approach to retain only one attribute from each opposing pair in the regression model.\n\n\n\nCorrelation Matrix Heatmap: Illuminating the Interdependence of Physical, Demographic, and Personality Traits in Relation to Total Pain\n\n\n\n\n1.2.3 One-Hot Encoding and Define Pipeline\nThe final stride in data preprocessing entailed the one-hot encoding of categorical variables such as ‘SEX’, ‘ACTIVITY LEVEL’, and ‘POSTURE’. Utilizing StringIndexer to convert strings to numerical indices and OneHotEncoder to map these indices to binary vectors, we constructed and integrated a pipeline to ensure the procedure is replicable for future datasets. The original categorical levels and their encoded counterparts are explicitly detailed for full transparency.\n\n\nCode\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml import Pipeline\n\n# Define the columns to be one-hot encoded\ncategoricalColumns = [\"SEX\", \"ACTIVITY LEVEL\",\"POSTURE\"]\n\n# Define the stages of the pipeline\nstages = []\n\n# Create and display the mappings for each categorical column\nfor categoricalCol in categoricalColumns:\n    # StringIndexer: convert strings to label indices\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    \n    # Fit and transform to display the mapping\n    indexed = stringIndexer.fit(health).transform(health)\n    indexed.select(categoricalCol, categoricalCol + \"Index\").distinct().show()\n    \n    # OneHotEncoder: encode label indices to binary vectors\n    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"OHE\"])\n    \n    stages += [stringIndexer, encoder]\n\n# Apply the stages in a pipeline to transform the DataFrame\npipeline = Pipeline(stages=stages)\npipelineModel = pipeline.fit(health)\ndf_transformed = pipelineModel.transform(health)\n\npipeline_model_path = \"Users/ml2078/fall-2023-reddit-project-team-10/pipeline\"\npipelineModel.save(pipeline_model_path)\n\n\n\n\n\nOne-Hot Encoded Columns \n\n\nSEXOHE\nACTIVITY LEVELOHE\nPOSTUREOHE\n\n\n\n\n(1,[0],[1.0])\n(2,[0],[1.0])\n(3,[1],[1.0])\n\n\n(1,[],[])\n(2,[],[])\n(3,[0],[1.0])\n\n\n(1,[],[])\n(2,[],[])\n(3,[1],[1.0])\n\n\n(1,[],[])\n(2,[1],[1.0])\n(3,[2],[1.0])\n\n\n(1,[0],[1.0])\n(2,[1],[1.0])\n(3,[1],[1.0])\n\n\n\n\n\n\n\nTable 1: One-Hot Encoding Mapping\n\n\n\n\n(a) Sex\n\n\nSex\nSEXIndex\n\n\n\n\nMale\n1.0\n\n\nFemale\n0.0\n\n\n\n\n\n\n(b) ACTIVITY LEVEL\n\n\nACTIVITY LEVEL\nACTIVITY LEVEL Index\n\n\n\n\nModerate\n1.0\n\n\nLow\n0.0\n\n\nHigh\n2.0\n\n\n\n\n\n\n(c) POSTURE\n\n\nPOSTURE\nPOSTURE index\n\n\n\n\nA\n1.0\n\n\nB\n0.0\n\n\nC\n3.0\n\n\nD\n2.0"
  },
  {
    "objectID": "ml.html#build-linear-regression-model",
    "href": "ml.html#build-linear-regression-model",
    "title": "Milestone 3 ML",
    "section": "1.3 Build Linear Regression Model",
    "text": "1.3 Build Linear Regression Model\nTo construct our linear regression model, we began by identifying the predictors: [“AGE”, “HEIGHT”, “WEIGHT”, “SEX_OHE”, “ACTIVITY_LEVEL_OHE”, “E”, “S”, “T”, “J”, “POSTURE_OHE”]. It’s crucial to note that for categorical variables such as ‘SEX’, ‘ACTIVITY LEVEL’, and ‘POSTURE’, we utilized their one-hot encoded representations to fit the requirements of the model for vector inputs. We then used the VectorAssembler to combine these features into a singular vector column.This process ensured that the data for each observation were consolidated into extensive vectors, ready for model ingestion.\nFollowing this, we partitioned the dataset, allocating 80% for training and reserving 20% for testing purposes, in order to evaluate the model’s performance on unseen data. The coefficients derived from the model are summarized in the table below, indicating the influence of each predictor on the total pain outcome.\n\n\nCode\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\n# Define the columns to be used as features\nfeatureCols = [\"AGE\", \"HEIGHT\", \"WEIGHT\", \"SEXOHE\", \"ACTIVITY LEVELOHE\", \"E\", \"S\", \"T\", \"J\",\"POSTUREOHE\"]\n\n# Assemble the features into a single vector column\nassembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\nassembled_df = assembler.transform(df_transformed)\n\n# Show the header of the new DataFrame\nassembled_df.select(\"features\").show(truncate=False)\n\n# Split the data into training and test sets\ntrainData, testData = assembled_df.randomSplit([0.8, 0.2], seed=42)\n# Show the header of the training and test data\ntrainData.select(\"features\").show(truncate=False)\ntestData.select(\"features\").show(truncate=False)\n\n# Define the regression model\nlr = LinearRegression(labelCol=\"total_pain\")\n\n# Train the model\nlrModel = lr.fit(trainData)\n\n# Print the coefficients and intercept for linear regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))\n\n\n\n\nTable 2: Coefficient Table Summary\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\nAGE\n0.031796\n\n\nHEIGHT\n-0.199597\n\n\nWEIGHT\n0.0431345\n\n\nSEX_MALE\n3.25555\n\n\nACTIVITY LEVEL MODERATE\n-2.88487\n\n\nACTIVITY LEVEL HIGH\n0.607446\n\n\nE\n0.057833\n\n\nS\n-0.183629\n\n\nT\n0.135234\n\n\nJ\n0.215914\n\n\nPOSTURE_A\n1.38215\n\n\nPOSTURE_D\n-4.28525\n\n\nPOSTURE_C\n3.40912\n\n\nIntercept\n14.0888\n\n\n\n\n\n\n\n\n1.3.1 Interpretations on the coefficients\n\nAge: The positive coefficient (0.0318) suggests that as age increases, so does the total pain score, albeit slightly.This indicates a gradual increase in pain with aging.\nHeight: The negative coefficient (-0.1996) implies that taller individuals are likely to report lower pain scores, possibly due to biomechanical advantages or differences in body composition.\nWeight: With a coefficient of 0.0431, there’s a slight positive association, indicating heavier individuals might experience more pain, which could be attributed to the additional stress on the body.\nSex (Male): The coefficient for males is significantly positive (3.2555), indicating that being male is associated with a higher pain score compared to the baseline of being female in this model. This could reflect differences in pain perception or reporting between genders.\nActivity Level: Participants with a ‘Moderate’ activity level have a negative coefficient (-2.8849), suggesting they report less pain compared to those with a ‘Low’ activity level, potentially due to better physical conditioning. Conversely, a ‘High’ activity level has a positive but smaller effect (0.6074), indicating increased pain, which might be due to more intense physical exertion.\nMBTI Traits (E, S, T, J):\n\n‘E’ (Extraversion) has a small positive coefficient (0.0578), hinting that more extraverted individuals might experience slightly more pain.\n‘S’ (Sensing) shows a negative relationship (-0.1836) with total pain, which might suggest that individuals who are more grounded in sensory experience report less pain.\n‘T’ (Thinking) has a positive coefficient (0.1352), which could imply a cognitive association with experiencing more pain.\n‘J’ (Judging) also has a positive coefficient (0.2159), possibly indicating that those with a more structured lifestyle might report higher pain levels.\n\nPosture:\n\n‘Posture A’ has a positive coefficient (1.3821), suggesting that this particular posture correlates with higher pain scores.\n‘Posture D’, however, is associated with a significantly lower pain score (-4.2853), indicating that it might be a more comfortable or ergonomically favorable posture.\n‘Posture C’ has the highest positive coefficient (3.4091), indicating a strong association with increased pain levels."
  },
  {
    "objectID": "ml.html#model-performance-evaluation",
    "href": "ml.html#model-performance-evaluation",
    "title": "Milestone 3 ML",
    "section": "1.4 Model Performance Evaluation",
    "text": "1.4 Model Performance Evaluation\nIn the final stage of our model assessment, we scrutinized the regression’s efficacy through two pivotal metrics: the Root Mean Squared Error (RMSE) and the coefficient of determination, R2. The RMSE for our training data was recorded at 6.42895, which, as anticipated, is lower than the RMSE for our testing data, measured at 9.91801. This discrepancy is not uncommon, as models tend to perform better on the data they were trained on due to their familiarity with the dataset’s nuances. More crucially, the R2 value for our model stands at 0.2364, signifying that approximately 23.64% of the variance in the dependent variable—total pain—is accounted for by the independent variables in our model.\n\n\nCode\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Make predictions\npredictions = lrModel.transform(testData)\n\n# Show some predictions\npredictions.select(\"prediction\", \"total_pain\", \"features\").show(5)\n\n# Evaluate the model\nevaluator = RegressionEvaluator(labelCol=\"total_pain\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\n\n\n# Make predictions\npredictions = lrModel.transform(trainData)\n\n# Show some predictions\npredictions.select(\"prediction\", \"total_pain\", \"features\").show(5)\n\n# Evaluate the model\nevaluator = RegressionEvaluator(labelCol=\"total_pain\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\n\n# Make predictions\npredictions = lrModel.transform(trainData)\n# Create an instance of RegressionEvaluator for evaluating R2\nevaluator = RegressionEvaluator(labelCol=\"total_pain\", predictionCol=\"prediction\", metricName=\"r2\")\n\n# Compute R2 on test data\nr2 = evaluator.evaluate(predictions)\n\n\n\n\n\n\n\nTrain Predictions \n\n\n\n\n\n\n\nfeatures\ntotal_pain\nprediction\n\n\n\n\n[53.0,62.0,125.0,1.0,1.0,0.0,18.0,17.0,9.0,18.0,0.0,1.0,0.0]\n0\n7.899\n\n\n[52.0,69.0,157.0,0.0,0.0,0.0,6.0,14.0,21.0,13.0,1.0,0.0,0.0]\n23\n13.5472\n\n\n[51.0,66.0,175.0,0.0,0.0,1.0,6.0,21.0,13.0,19.0,0.0,0.0,1.0]\n30\n16.4532\n\n\n[45.0,63.0,199.0,1.0,0.0,1.0,14.0,20.0,9.0,16.0,0.0,1.0,0.0]\n13\n12.9153\n\n\n[68.0,74.0,182.0,0.0,1.0,0.0,4.0,17.0,11.0,4.0,0.0,0.0,1.0]\n4\n9.31628\n\n\n\n\n\n\n\n\n\n\nTest Predictions \n\n\n\n\n\n\n\nfeatures\ntotal_pain\nprediction\n\n\n\n\n[30.0,69.0,200.0,0.0,0.0,0.0,15.0,16.0,15.0,12.0,0.0,1.0,0.0]\n0\n8.16098\n\n\n[62.0,68.0,263.0,0.0,1.0,0.0,7.0,20.0,14.0,9.0,1.0,0.0,0.0]\n37\n12.8979\n\n\n[66.0,67.0,180.0,0.0,1.0,0.0,19.0,18.0,11.0,13.0,0.0,0.0,0.0]\n14\n9.78157\n\n\n[57.0,68.0,185.0,0.0,1.0,0.0,16.0,12.0,15.0,17.0,1.0,0.0,0.0]\n17\n13.2265\n\n\n[23.0,65.0,110.0,1.0,1.0,0.0,13.0,15.0,12.0,9.0,1.0,0.0,0.0]\n13\n9.90728\n\n\n\n\n\n\n\n\n\nRoot Mean Squared Error (RMSE) on train data = 6.42895\nRoot Mean Squared Error (RMSE) on test data = 9.91801\nR2 = 0.2364"
  },
  {
    "objectID": "ml.html#apply-model-to-new-data",
    "href": "ml.html#apply-model-to-new-data",
    "title": "Milestone 3 ML",
    "section": "1.5 Apply Model to New Data",
    "text": "1.5 Apply Model to New Data\nUpon successfully training our linear regression model with the original dataset, we proceeded to serialize and store the model for future use. When faced with new, unseen data, our first step was to construct five novel data points, ensuring consistency in variable naming conventions as per the trained model’s specifications.\n\n\n\nNew Data \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAGE\nHEIGHT\nWEIGHT\nSEX\nACTIVITY_LEVEL\nE\nS\nT\nJ\nPOSTURE\n\n\n\n\n50\n62\n140\nMale\nModerate\n12\n15\n20\n5\nB\n\n\n30\n66\n100\nFemale\nLow\n18\n12\n15\n8\nC\n\n\n35\n71\n160\nMale\nHigh\n10\n3\n10\n3\nA\n\n\n44\n63\n130\nMale\nHigh\n6\n9\n8\n10\nD\n\n\n48\n61\n120\nFemale\nLow\n3\n17\n5\n20\nB\n\n\n\n\n\nSubsequently, we utilized the previously established pipeline—now recalled from storage—to efficiently one-hot encode the categorical variables within this fresh dataset. The use of this pre-defined pipeline not only preserved coherence in data processing but also proved to be a time-saving measure. With the new data duly processed, we used the VectorAssembler once again to collate the features into a single vector column, thereby preparing the data in the exact format required for our model. We then fed this prepared vector into our pre-trained linear regression model to generate predictions. The model’s predictions, which are the inferred pain levels based on the input features, are as follows:\n\n\nCode\nfrom pyspark.sql import Row\nfrom pyspark.ml import Pipeline\n\n# Example new data rows\nnew_data_rows = [\n    Row(AGE=50, HEIGHT=62, WEIGHT=140, SEX=\"Male\", ACTIVITY_LEVEL=\"Moderate\", E=12, S=15, T=20, J=5,POSTURE=\"B\"),\n    Row(AGE=30, HEIGHT=66, WEIGHT=100, SEX=\"Female\", ACTIVITY_LEVEL=\"Low\", E=18, S=12, T=15, J=8,POSTURE=\"C\"),\n    Row(AGE=35, HEIGHT=71, WEIGHT=160, SEX=\"Male\", ACTIVITY_LEVEL=\"High\", E=10, S=3, T=10, J=3,POSTURE=\"A\"),\n    Row(AGE=44, HEIGHT=63, WEIGHT=130, SEX=\"Male\", ACTIVITY_LEVEL=\"High\", E=6, S=9, T=8, J=10,POSTURE=\"D\"),\n    Row(AGE=48, HEIGHT=61, WEIGHT=120, SEX=\"Female\", ACTIVITY_LEVEL=\"Low\", E=3, S=17, T=5, J=20,POSTURE=\"B\"),\n]\n\n# Create a DataFrame with the new data\nnew_data_df = spark.createDataFrame(new_data_rows)\n\n# Load the pipeline model\nloaded_pipeline_model = PipelineModel.load(\"Users/ml2078/fall-2023-reddit-project-team-10/pipeline\")\n\n# Use the loaded pipeline model to transform new data\ndf_transformed_new = loaded_pipeline_model.transform(new_data_df)\n\n\n\n\n\nNew Data Predictions \n\n\n\n\n\n\nfeatures\nprediction\n\n\n\n\n[50.0,62.0,140.0,0.0,0.0,1.0,12.0,15.0,20.0,5.0,1.0,0.0,0.0]\n13.0558\n\n\n[30.0,66.0,100.0,1.0,1.0,0.0,18.0,12.0,15.0,8.0,0.0,0.0,0.0]\n9.14661\n\n\n[35.0,71.0,160.0,0.0,0.0,0.0,10.0,3.0,10.0,3.0,0.0,1.0,0.0]\n5.67401\n\n\n[44.0,63.0,130.0,0.0,0.0,0.0,6.0,9.0,8.0,10.0,0.0,0.0,1.0]\n13.8651\n\n\n[48.0,61.0,120.0,1.0,1.0,0.0,3.0,17.0,5.0,20.0,1.0,0.0,0.0]\n12.4147"
  },
  {
    "objectID": "ml.html#analysis-goal-1",
    "href": "ml.html#analysis-goal-1",
    "title": "Milestone 3 ML",
    "section": "2.1 Analysis Goal",
    "text": "2.1 Analysis Goal\nIn this study, we aim to delve into MBTI-related comments on Reddit and external data with the aim of revealing the extent to which different MBTI personality types are associated with each other in everyone’s discussions. At the core of this analytical journey is the exploration of the interconnections between communication styles and expressions specific to various personality types in the digital environment. By employing FP-Growth modeling to dissect and analyze text data on Reddit and external data, our goal is not only to identify frequent item sets and association rules between MBTI types, but also to understand how these traits influence online conversations. The ultimate goal is to present these relationships through intuitive association rule network graphs, leading to a deeper understanding of how different personality types converge and diverge in the realm of digital communication.\nLink to Association Rules Analysis Notebook Code"
  },
  {
    "objectID": "ml.html#data-preprocessing-1",
    "href": "ml.html#data-preprocessing-1",
    "title": "Milestone 3 ML",
    "section": "2.2 Data Preprocessing",
    "text": "2.2 Data Preprocessing\n\n2.2.1 Text Preprocessing\nIn the text preprocessing, we focus on extracting MBTI type-related information from the textual content of comments on Reddit as well as from external data sources. By identifying keywords associated with the 16 MBTI types, we have created a new column titled “mbti_type_related,” which will serve as the foundation for subsequent association rule learning. During this process, for comments that do not contain any identifiable MBTI type information, we have designated their “mbti_type_related” column as “general.” We will further filter the data in subsequent steps to ensure that each comment in our final dataset mentions at least one or more MBTI types.\n\n\nCode\nfrom pyspark.sql.functions import udf, when,col\nfrom pyspark.sql.types import StringType\nimport re\n\n# List of personality types in uppercase\npersonality_types = [\"ESTJ\", \"ISTJ\", \"INFP\", \"ENFP\", \"INTJ\", \"ENTJ\", \"INTP\", \"ENTP\",\n                    \"ESFJ\", \"ISFJ\", \"ENFJ\", \"INFJ\", \"ESFP\", \"ISFP\", \"ISTP\", \"ESTP\"]\n\n# Convert the list to a regex pattern with case-insensitive flag\npattern = \"(?i)\\\\b(\" + \"|\".join(personality_types) + \")\\\\b\"\n\n# Define UDF to extract all matches\ndef extract_all_types(title):\n    matches = re.findall(pattern, title, re.IGNORECASE)\n    # Convert matches to uppercase\n    matches_upper = [match.upper() for match in matches]\n    return ', '.join(matches_upper)\n\nextract_all_types_udf = udf(extract_all_types, StringType())\n\n# Apply UDF to get all MBTI types\ncomment_load = comment_load.withColumn(\"mbti_type_related_temp\", extract_all_types_udf(col(\"comment_text\")))\n\n# Set 'mbti_type_related' to 'general' for empty matches\ncomment_load = comment_load.withColumn(\"mbti_type_related\",\n                                         when(col(\"mbti_type_related_temp\") == \"\", \"general\")\n                                         .otherwise(col(\"mbti_type_related_temp\")))\n\n# Drop the temporary column\ncomment_load = comment_load.drop(\"mbti_type_related_temp\")\n\n\nFollowing the initial preprocessing of the data, the focus of this phase is on the preliminary presentation of the processed data. As demonstrated by the results, we have successfully extracted MBTI-related information from the textual content. The completion of this step marks a significant milestone in our data processing workflow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsub_id\ncomment_author\ncomment_text\nlink_id\ncomment_score\ncomment_controversiality\nreply_to\nyear\nmonth\nmbti_type_related\n\n\n\n\nrfzn00\nMaster-Elk-5465\nyes it feels like i’m finally understanding myself and knowing that i’m not the only one who feels this way🥰\nt3_rfzn00\n7\n0\nt3\n2021\n12\ngeneral\n\n\nrfuyza\nPragmaticGuardian613\nHahaha! What?????\nt3_rfuyza\n1\n0\nt1\n2021\n12\ngeneral\n\n\nrezbts\n[deleted]\n[deleted]\nt3_rezbts\n2\n0\nt1\n2021\n12\ngeneral\n\n\nrfmj3f\nGiveretLivni\nI’d photo my friends through the window while they were asleep and put the photos in their notebooks.\nt3_rfmj3f\n1\n0\nt3\n2021\n12\ngeneral\n\n\n\n\n\nIn this stage, we apply the same data preprocessing approach to the external dataset that was used for the Reddit dataset. Consistent with the procedure for the Reddit data, we have also created a column named “mbti_type_related” for the external dataset. This approach is intended to facilitate the seamless merging of these two datasets in subsequent steps.\n\n\n2.2.2 Merge the Reddit and extrual data\nIn this step, we merge the two datasets and filter out only the columns necessary for the subsequent association rule learning process, discarding all irrelevant data. This action is aimed at optimizing our analysis process, allowing us to more effectively explore the correlations between various MBTI types in data sourced from multiple origins. By integrating different datasets, we not only enhance the comprehensiveness of our analysis but also improve its overall interpretability. As of now, the consolidated dataset for association rule learning comprises a total of 540,780 rows.\n\n\nCode\nfrom pyspark.sql import SparkSession\n\n# Convert Pandas DataFrame to PySpark DataFrame\nmbti_in_post_spark = spark.createDataFrame(mbti_in_post_df)\n\nmbti_in_post_spark.show()\n\n#merge the data by column name\ncombined_df = arm_comment.unionByName(mbti_in_post_spark)\ncombined_df.printSchema()\n\n#print the number of the row\nrow_count = combined_df.count()\nprint(\"Number of rows:\", row_count)\n\n\n\n\n\n\n\nUnnamed: 0\ncomment_text\nmbti_type_related\n\n\n\n\n0\nI think you may be an ENFP Ne: dominant function; Fi: Auxiliary function; Te: 3rd function; Si: weak function; It is because you scored low on consciousness so you are out of the xxxJ and high on openness and agreeableness then you are probably xNFP and since you got 65% Extroversion then it should be ENFP You could use this site to test your personality type again. it was more accurate for me than most other websites. careerplanner MBTI free test\nENFP, ENFP\n\n\n1\nisfp\nISFP\n\n\n2\nENTJ, or ESTJ\nENTJ, ESTJ\n\n\n3\nENFP. (Sing in Bill Nye the science guy’s theme) I’m GC the human guy G C G C GC the human guy\nENFP\n\n\n4\nIntj\nINTJ"
  },
  {
    "objectID": "ml.html#build-fp-growth-model",
    "href": "ml.html#build-fp-growth-model",
    "title": "Milestone 3 ML",
    "section": "2.3 Build FP-Growth Model",
    "text": "2.3 Build FP-Growth Model\nIn the FP-Growth model construction phase, our main goal is to utilize the FP-Growth algorithm to deeply explore and identify frequent itemsets and association rules in MBTI-type correlation data. FP-Growth, or Frequent Pattern Growth Algorithm, is an efficient method to mine frequent itemsets, which has the advantage of avoiding the generation of candidate itemsets, thus significantly reducing the computational amount of computation. This algorithm is especially suitable for processing large data sets and can effectively discover patterns and relationships in the data.\nWhen applying the FP-Growth algorithm, we will first set appropriate thresholds for minSupport and minConfidence. Support is used to measure how often the itemset appears in all transactions, while confidence measures the reliability of the rule. With these parameters, we are able to filter out less significant itemsets and rules, thus focusing on analyzing those patterns that are most statistically significant.\nThe algorithm will then identify frequent itemsets in the dataset and generate association rules based on these itemsets. These rules will help us understand how different MBTI types combine and correlate, revealing the underlying relationships behind user behaviors and tendencies. For example, we may find that specific combinations of MBTI types tend to appear frequently in discussions, or that certain types of people are more inclined to discuss specific topics.\n\n\nCode\nfrom pyspark.sql.functions import split\n\n# Split strings into lists\ncombined_df = combined_df.withColumn(\"mbti_type_related\", split(combined_df[\"mbti_type_related\"], \", \"))\n\nfrom pyspark.sql.functions import array_distinct\ncombined_df = combined_df.withColumn(\"mbti_type_related\", array_distinct(\"mbti_type_related\"))\n\nfrom pyspark.ml.fpm import FPGrowth\n\n# Create FP-Growth models\nfpGrowth = FPGrowth(itemsCol=\"mbti_type_related\", minSupport=0.01, minConfidence=0.1)\n\n# Training models\nmodel = fpGrowth.fit(combined_df)\n\n# View frequent itemsets\nmodel.freqItemsets.show()\n\n# check the associationRules\nmodel.associationRules.show()\n\n\nSince throughout our session, we preferred to discover the probability of another item occurring if the ISTP(or others) is known to occur, we use confidence for ranking here.\n\n\nCode\nfrom pyspark.sql.functions import col\n\n# Sorted in descending order of confidence\nconf_rules = model.associationRules.orderBy(col(\"confidence\").desc())\n\n# Show these rules\nconf_rules.show()\n\n# Sorted in descending order of lift\nlift_rules = model.associationRules.orderBy(col(\"lift\").desc())\n\n# Show these rules\nlift_rules.show()\n\n\nBut lift is also a good measure of whether this rule is valid and really relevant, so check this in descending order as well.\n\n\n\nAssociation rules order by Confidence \n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nantecedent\nconsequent\nconfidence\nlift\nsupport\n\n\n\n\n0\n[‘ISFP’]\n[‘INFP’]\n0.235145\n1.31319\n0.0135674\n\n\n1\n[‘ENFJ’]\n[‘INFJ’]\n0.202899\n1.40551\n0.0115944\n\n\n2\n[‘ESTP’]\n[‘ENTP’]\n0.198977\n1.48417\n0.0107863\n\n\n3\n[‘ISTJ’]\n[‘INTJ’]\n0.195975\n1.26531\n0.0110932\n\n\n4\n[‘ENTJ’]\n[‘INTJ’]\n0.187587\n1.21114\n0.0147713\n\n\n\n\n\n\n\n\nAssociation rules order by Lift \n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nantecedent\nconsequent\nconfidence\nlift\nsupport\n\n\n\n\n0\n[‘ESTP’]\n[‘ENTP’]\n0.198977\n1.48417\n0.0107863\n\n\n1\n[‘ENFJ’]\n[‘INFJ’]\n0.202899\n1.40551\n0.0115944\n\n\n2\n[‘ISFP’]\n[‘INFP’]\n0.235145\n1.31319\n0.0135674\n\n\n3\n[‘ISTJ’]\n[‘INTJ’]\n0.195975\n1.26531\n0.0110932\n\n\n4\n[‘ENTJ’]\n[‘INTJ’]\n0.187587\n1.21114\n0.0147713\n\n\n\n\n\nA brief description of these indicators and an analysis of the first five rules in the dataset are presented below:\nConfidence: this indicates the conditional probability of the occurrence of the latter item when the former item occurs. For example, the first rule has a confidence level of 0.235145, which means that when ‘ISFP’ occurs, there is about a 23.51% probability that ‘INFP’ will also occur.\nLift: A lift greater than 1 means that there is a positive correlation between the antecedent and the consequent, i.e., they tend to occur together, and that this correlation is stronger than the probability of random occurrence. For example, the first rule has a lift of 1.313194, indicating that ‘ISFP’ and ‘INFP’ occur together more often than ‘INFP’ occurs alone.\nSupport: This indicates how often the combination of the antecedent and the consequent occurs in all transactions. For example, the first rule has a support of 0.013567, indicating that ‘ISFP’ and ‘INFP’ together occur 1.3567% of all possible combinations.\nThe first five rules are specifically analyzed:\nRule 1: ISFP -> INFP\n\nThe high confidence level indicates that there is a relatively high probability that INFP will be mentioned in a discussion that mentions ISFP.\n\nRule 2: ENFJ -> INFJ\n\nThe highest elevation means that ENFJ and INFJ occur together much more often than the random probability, suggesting that there may be a strong correlation between them.\n\nRule 3: ESTP -> ENTP\n\nThe elevation is also high, suggesting that ESTP and ENTP may have a tendency to appear together in discussions.\n\nRule 4: ISTJ -> INTJ\n\nBoth confidence and elevation suggest a correlation between ISTJ and INTJ.\n\nRule 5: ENTJ -> INTJ\n\nThis rule shows that ENTJs and INTJs also occur together relatively frequently, and that when ENTJs occur, INTJs are also likely to occur.\n\nThese rules can help us understand how certain personality types tend to be mentioned alongside others in MBTI discussions. For example, Sensing and Intuitive types may appear together in some discussions, reflecting the diversity of the discussion or the complementarity of these types in a given context. Similarly, the combination of Extraversion and Introversion may indicate discussions focused on specific social or introspective themes."
  },
  {
    "objectID": "ml.html#association-rule-network-graphs",
    "href": "ml.html#association-rule-network-graphs",
    "title": "Milestone 3 ML",
    "section": "2.4 Association Rule Network Graphs",
    "text": "2.4 Association Rule Network Graphs\nFrom the data extracted from the model, we have obtained two tables sorted by different criteria: one sorted by confidence and the other by lift. Given our research focus is on exploring the probability of the occurrence of one MBTI type given the presence of another, we have opted to use the table sorted by confidence for the subsequent construction of the network graph. This approach allows us to more accurately analyze and understand the direct correlations between different MBTI types.\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objs as go\nimport networkx as nx\n\nrules_pd=pd.read_csv(\"../data/csv/ordered_association_rules.csv\")\n\n# Create a graph\nG = nx.DiGraph()\n\n# Add nodes and edges\nfor _, row in rules_pd.iterrows():\n    G.add_edge(str(row['antecedent']), str(row['consequent']), weight=row['confidence'])\n\n# Generate position layout\npos = nx.spring_layout(G)\n\n# Create edge trace\nedge_x = []\nedge_y = []\nfor edge in G.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.extend([x0, x1, None])\n    edge_y.extend([y0, y1, None])\n\nedge_trace = go.Scatter(\n    x=edge_x, y=edge_y,\n    line=dict(width=0.5, color='#888'),\n    hoverinfo='none',\n    mode='lines')\n\n# Create node trace\nnode_x = []\nnode_y = []\nfor node in G.nodes():\n    x, y = pos[node]\n    node_x.append(x)\n    node_y.append(y)\n\n#color_list=['#ffffd9', '#f5fbc4', '#eaf7b1', '#d6efb3', '#bde5b5', '#97d6b9', '#73c8bd', '#52bcc2', '#37acc3', '#2498c1', '#1f80b8', '#2165ab', '#234da0', '#253795', '#172978', '#081d58']\ncolor_list=['#081d58','#253795','#1f80b8','#97d6b9','#ffffd9']\n\nnode_trace = go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers+text',  # Add 'text' to the mode\n    text=[node for node in G.nodes()],  # Add node labels\n    textposition=\"bottom center\",  # Position of text\n    hoverinfo='text',\n    marker=dict(\n        showscale=True,\n        colorscale=color_list,\n        reversescale=True,\n        color=[],\n        size=15,  # Increase node size\n        colorbar=dict(\n            thickness=15,\n            title='Number of Node Connections',\n            xanchor='left',\n            titleside='right'\n        ),\n        line_width=1.5))\n\n# Add node text and hover info\nnode_adjacencies = []\nnode_text = []\nfor node, adjacencies in enumerate(G.adjacency()):\n    node_adjacencies.append(len(adjacencies[1]))\n    node_text.append(f'{adjacencies[0]}')\n\nnode_trace.marker.color = node_adjacencies\nnode_trace.text = node_text\n\n# Create figure\nfig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title='<br>Network graph of association rules',\n                titlefont_size=23,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=80),\n                annotations=[dict(\n                    text=\"Python plotly library\",\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002)],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\n\nfig.show()\n\n\n\n                                                \n\n\nFrom the network graph presented, it can be observed that the ENFP type has the highest degree of connectivity with other MBTI types, reaching a value of 5. Conversely, the ISFP, ESTP, and ISTJ types exhibit lower levels of connectivity. This finding indicates that within the spectrum of the 16 MBTI types, the ENFP type is relatively more active and is frequently mentioned in conjunction with other types during MBTI-related discussions. On the other hand, the ISFP, ISTJ, and ESTP types are less often mentioned simultaneously, reflecting a lower degree of connectivity."
  },
  {
    "objectID": "ml.html#analysis-goal-2",
    "href": "ml.html#analysis-goal-2",
    "title": "Milestone 3 ML",
    "section": "3.1 Analysis Goal",
    "text": "3.1 Analysis Goal\nDevelop a predictive model that can identify an individual’s MBTI personality type based on their online text entries.\n\nClean the text data to remove irrelevant information, such as special characters, URLs, and non-standard language elements.\nApply natural language processing techniques to tokenize, and remove stopwords from the text data for further analysis.\nTrain machine learning models (e.g., logistic regression, support vector machines, random forests to predict MBTI types based on the engineered features.\nDefine clear metrics to compare model performances, focusing on accuracy, F1 score, Precision, and Recall to measure the model’s ability to classify the posts into different MBTI types.\nPlot confusion matrices to analyze the model’s performance in predicting each MBTI type.\n\nLink to Predictive Model Analysis Notebook Code"
  },
  {
    "objectID": "ml.html#data-preprocessing-2",
    "href": "ml.html#data-preprocessing-2",
    "title": "Milestone 3 ML",
    "section": "3.2 Data Preprocessing",
    "text": "3.2 Data Preprocessing\nThe original external dataset contains two columns which are the MBTI type and the posts.\n\n\nCode\nimport IPython.display as d\ndata_or = pd.read_csv(\"../data/csv/mbti_1.csv\")\nmd = tabulate(data_or.head(),headers='keys',tablefmt='pipe',showindex=False)\nd.Markdown(md) \n\n\n\n\n\n\n\n\n\ntype\nposts\n\n\n\n\nINFJ\n’http://www.youtube.com/watch?v=qsXHcwe3krw\n\n\nENTP\n’I’m finding the lack of me in these posts very alarming.\n\n\nINTP\n’Good one _____ https://www.youtube.com/watch?v=fHiGbolFFGw\n\n\nINTJ\n’Dear INTP, I enjoyed our conversation the other day. Esoteric gabbing about the nature of the universe and the idea that every rule and social code being arbitrary constructs created…\n\n\nENTJ\n’You’re fired.\n\n\n\n\n\nAs the posts are mainly text data, the first step is to clean the text data. We use the following steps to clean the text data：\n\nConvert the text data to lower case\nRemove the punctuation\nRemove the rows with na in the cleaned_text column\n\n\n\nCode\nimport IPython.display as d\ndata_pd = pd.read_csv(\"../data/csv/mbti_1.csv\")\ndata = spark.createDataFrame(data_pd)\n#convert to lower case\ndf_cleaned = data.withColumn(\"cleaned_text\", lower(col(\"posts\")))\n# remove punctuation\ndf_cleaned = df_cleaned.withColumn(\"cleaned_text\", regexp_replace(\"cleaned_text\", \"[^a-zA-Z0-9\\\\s]\", \"\"))\n# remove the rows with na in the cleaned_text column\ndf_cleaned = df_cleaned.na.drop(subset=[\"cleaned_text\"])"
  },
  {
    "objectID": "ml.html#build-the-pipeline-for-the-classification-model",
    "href": "ml.html#build-the-pipeline-for-the-classification-model",
    "title": "Milestone 3 ML",
    "section": "3.3 Build the pipeline for the classification model",
    "text": "3.3 Build the pipeline for the classification model\nIn order to build the classification model, we need to convert the text data into numeric data. We use the following steps to convert the text data into numeric data and we also split the data into training, testing, and validation sets. Besides, we also use Tokenizer and CountVectorizer to tokenize and vectorize the text data. We use the StringIndexer to convert the MBTI type column to a numeric type.\n\n\nCode\n#load all the packages\n# Tokenize and Vectorize Text\ntokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\nvectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\")\n#  Convert the \"type\" column to a numeric type\nindexer = StringIndexer(inputCol=\"type\", outputCol=\"label\")\n# Split Data into Train, Test, and Validation Sets\n# Adjust the ratios based on your preference\ntrain_df, test_df, val_df = df_cleaned.randomSplit([0.8, 0.1, 0.1], seed=42)\n\n\n\nBuild Logistic Regression Model, Support Vector Machine Model and Random Forest Model\nThe first model we would like to build is a simple model, which is logistic regression model. We use the pipeline to build the models. The second model we want to build is a support vector machine model. As we have 16 labels in our task, we use the OneVsRest method to build the model. The last model we build is a random forest classifier model. We use the default parameters for the model.\n\n\nCode\n# Logistic Regression Classifier\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\nlr_pipeline = Pipeline(stages=[tokenizer, vectorizer,indexer, lr])\nlr_model = lr_pipeline.fit(train_df)\nlsvc = LinearSVC(maxIter=10)\novr = OneVsRest(classifier=lsvc, labelCol=\"label\", featuresCol=\"features\")\novr_pipeline = Pipeline(stages=[tokenizer, vectorizer, indexer, ovr])\novr_model = ovr_pipeline.fit(train_df)\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\nrf_pipeline = Pipeline(stages=[tokenizer, vectorizer, indexer, rf])\nrf_model = rf_pipeline.fit(train_df)"
  },
  {
    "objectID": "ml.html#model-evaluation",
    "href": "ml.html#model-evaluation",
    "title": "Milestone 3 ML",
    "section": "3.4 Model Evaluation",
    "text": "3.4 Model Evaluation\nAs the our task is a multi-class classification task, we use the accuracy, f1 score, precision and recall to evaluate the model performance.\n\n\nCode\n# Evaluate the Models\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\nf1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\nprecision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nrecall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n# Evaluate Logistic Regression model\nlr_test_predictions = lr_model.transform(test_df)\nlr_val_predictions = lr_model.transform(val_df)\nlr_test_accuracy = evaluator.evaluate(lr_test_predictions)\nlr_test_f1_score = f1_evaluator.evaluate(lr_test_predictions)\nlr_test_precision = precision_evaluator.evaluate(lr_test_predictions)\nlr_test_recall = recall_evaluator.evaluate(lr_test_predictions)\n\nlr_val_accuracy = evaluator.evaluate(lr_val_predictions)\nlr_val_f1_score = f1_evaluator.evaluate(lr_val_predictions)\nlr_val_precision = precision_evaluator.evaluate(lr_val_predictions)\nlr_val_recall = recall_evaluator.evaluate(lr_val_predictions)\n\n# Evaluate Support Vector model\novr_test_predictions = ovr_model.transform(test_df)\novr_val_predictions = ovr_model.transform(val_df)\novr_test_accuracy = evaluator.evaluate(ovr_test_predictions)\novr_test_f1_score = f1_evaluator.evaluate(ovr_test_predictions)\novr_test_precision = precision_evaluator.evaluate(ovr_test_predictions)\novr_test_recall = recall_evaluator.evaluate(ovr_test_predictions)\n\novr_val_accuracy = evaluator.evaluate(ovr_val_predictions)\novr_val_f1_score = f1_evaluator.evaluate(ovr_val_predictions)\novr_val_precision = precision_evaluator.evaluate(ovr_val_predictions)\novr_val_recall = recall_evaluator.evaluate(ovr_val_predictions)\n# Evaluate RF model\nrf_test_predictions = rf_model.transform(test_df)\nrf_val_predictions = rf_model.transform(val_df)\nrf_test_accuracy = evaluator.evaluate(rf_test_predictions)\nrf_test_f1_score = f1_evaluator.evaluate(rf_test_predictions)\nrf_test_precision = precision_evaluator.evaluate(rf_test_predictions)\nrf_test_recall = recall_evaluator.evaluate(rf_test_predictions)\n\nrf_val_accuracy = evaluator.evaluate(rf_val_predictions)\nrf_val_f1_score = f1_evaluator.evaluate(rf_val_predictions)\nrf_val_precision = precision_evaluator.evaluate(rf_val_predictions)\nrf_val_recall = recall_evaluator.evaluate(rf_val_predictions)\n\n\nThe final classification results are shown in the table below.\n\n\nCode\nimport pandas as pd\nres = pd.read_csv(\"../data/csv/ML_predict_results.csv\", index_col=0)\nmd = tabulate(res.head(),headers='keys',tablefmt='pipe',showindex=False)\nd.Markdown(md) \n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ndataset\naccuracy\nf1_score\nprecision\nrecall\n\n\n\n\nLogistic Regression\nTest\n0.374248\n0.334223\n0.358177\n0.374248\n\n\nLogistic Regression\nValidation\n0.369515\n0.3277\n0.334903\n0.369515\n\n\nSVM\nTest\n0.363418\n0.365791\n0.377451\n0.363418\n\n\nSVM\nValidation\n0.384527\n0.385806\n0.390006\n0.384527\n\n\nRandom Forest\nTest\n0.240674\n0.120616\n0.221018\n0.240674\n\n\n\n\n\nExamining the performance metrics from the table reveals distinctive strengths and weaknesses among the models. The logistic regression model stands out with the highest test accuracy, reaching approximately 37.4%. On the other hand, the SVM model attains the highest F1 score, primarily driven by its exceptional precision.In the realm of the validation set, the SVM model emerges as the top performer across multiple metrics, boasting the highest accuracy, F1 score, precision, and recall. This suggests a robust overall performance for the SVM model on the validation data.Conversely, the random forest model demonstrates suboptimal results on both the test and validation sets, indicating potential challenges in capturing the underlying patterns of the data."
  },
  {
    "objectID": "ml.html#confusion-matrix",
    "href": "ml.html#confusion-matrix",
    "title": "Milestone 3 ML",
    "section": "3.5 Confusion Matrix",
    "text": "3.5 Confusion Matrix\nIn order to analyze the classification results in more details, we plot the confusion matrix for the logistic regression model, SVM model and Random Forest model.\n\n3.5.1 Logistic Regression Confusion Matrix\n\n\nCode\nimport pandas as pd\nimport warnings\n# Suppress Matplotlib warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\nlr = pd.read_csv(\"../data/csv/lr_test_confusion_matrix.csv\",index_col=0)\novr = pd.read_csv(\"../data/csv/ovr_test_confusion_matrix.csv\",index_col=0)\nrf = pd.read_csv(\"../data/csv/rf_test_confusion_matrix.csv\",index_col=0)\nlabel_map = {0:'ENFJ',1:'ENFP',2:'ENTJ',3:'ENTP',4:'ESFJ',5:'ESFP',6:'ESTJ',7:'ESTP',8:'INFJ',9:'INFP',10:'INTJ',11:'INTP',12:'ISFJ',13:'ISFP',14:'ISTJ',15:'ISTP'}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nconfusion_matrix = pd.pivot_table(lr, values='count', index='label', columns='prediction', fill_value=0)\ncolor_list=['#ffffd9', '#f5fbc4', '#eaf7b1', '#d6efb3', '#bde5b5', '#97d6b9', '#73c8bd', '#52bcc2', '#37acc3', '#2498c1', '#1f80b8', '#2165ab', '#234da0', '#253795', '#172978', '#081d58']\n# Create a heatmap using seaborn\nplt.figure(figsize=(8, 6))\nheatmap = sns.heatmap(confusion_matrix, annot=True, cmap=sns.color_palette(color_list), fmt='g', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nheatmap.set_yticklabels([label_map[label] for label in confusion_matrix.index], rotation=0)\nheatmap.set_xticklabels([label_map[label] for label in confusion_matrix.columns], rotation=0)\nplt.title('Logitstic Regression Confusion Matrix Heatmap')\n#plt.savefig(\"../data/plots/LR_confusion_matrix.png\")\nplt.show()\n\n\n\n\n\nBased on the heatmap of the confusion matrix for the logistic regression model, notable performance trends emerge. The model demonstrates proficiency in accurately predicting labels for personality types such as ENFJ, ENFP, ENTJ, ESFJ, and ESFP, reflecting its robust performance in these cases. Conversely, the model exhibits shortcomings in predicting labels for ISFJ, ISFP, ISTJ, and ISTP, indicating suboptimal accuracy for these personality types.An inference can be drawn from these observations: the logistic regression model tends to achieve more accurate predictions for personality types falling under the EN category, while its accuracy diminishes when predicting personality types within the IS category. This discrepancy may arise from inherent differences in the linguistic patterns or content associated with EN and IS personalities, affecting the model’s ability to generalize effectively.\n\n\n3.5.2 SVM Confusion Matrix\n\n\nCode\nconfusion_matrix = pd.pivot_table(ovr, values='count', index='label', columns='prediction', fill_value=0)\ncolor_list=['#ffffd9', '#f5fbc4', '#eaf7b1', '#d6efb3', '#bde5b5', '#97d6b9', '#73c8bd', '#52bcc2', '#37acc3', '#2498c1', '#1f80b8', '#2165ab', '#234da0', '#253795', '#172978', '#081d58']\n# Create a heatmap using seaborn\nplt.figure(figsize=(8, 6))\nheatmap = sns.heatmap(confusion_matrix, annot=True, cmap=sns.color_palette(color_list), fmt='g', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nheatmap.set_yticklabels([label_map[label] for label in confusion_matrix.index], rotation=0)\nheatmap.set_xticklabels([label_map[label] for label in confusion_matrix.columns], rotation=30)\nplt.title('SVM Confusion Matrix Heatmap')\n#plt.savefig(\"../data/plots/SVM_confusion_matrix.png\")\nplt.show()\n\n\n\n\n\nThe confusion matrix heatmap for the SVM model reveals notable performance patterns across personality labels. Proficient predictions are observed for ENFJ, ENFP, ENTJ, ESFJ, and ESFP, indicating the model’s accuracy in these cases. Conversely, inadequate predictions are evident for ISFJ, ISFP, ISTJ, and ISTP, reflecting the model’s struggle with these labels. Remarkably, a recurring tendency emerges where IS types are consistently misclassified as EN types, mirroring a trend observed in the logistic regression model. This recurrent misclassification could be attributed to the inherent characteristics of IS types (Introversion and Sensing) being mistakenly associated with EN types (Extroversion and Intuition) by the model. Moreover, it’s noteworthy that the disparity in the number of posts between IS and EN types might contribute to the misclassification trend. The lower volume of posts from IS types could lead the model to exhibit a bias, as it may be more trained and inclined towards patterns observed in the more abundant EN type posts.\n\n\n3.5.3 Random Forest Confusion Matrix\n\n\nCode\nconfusion_matrix = pd.pivot_table(rf, values='count', index='label', columns='prediction', fill_value=0)\n#deine a color list\ncolor_list=['#ffffd9', '#f5fbc4', '#eaf7b1', '#d6efb3', '#bde5b5', '#97d6b9', '#73c8bd', '#52bcc2', '#37acc3', '#2498c1', '#1f80b8', '#2165ab', '#234da0', '#253795', '#172978', '#081d58']\n# Create a heatmap using seaborn\nplt.figure(figsize=(8, 6))\nheatmap = sns.heatmap(confusion_matrix, annot=True, cmap=sns.color_palette(color_list), fmt='g', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nheatmap.set_yticklabels([label_map[label] for label in confusion_matrix.index], rotation=0)\nheatmap.set_xticklabels([label_map[label] for label in confusion_matrix.columns], rotation=0)\nplt.title('Random Forest Confusion Matrix Heatmap')\n#plt.savefig(\"../data/plots/RF_confusion_matrix.png\")\nplt.show()\n\n\n\n\n\nThe plot indicates that the random forest model exhibits a tendency to classify all personality types into a limited set, specifically ENFJ, ENTP, ENTJ, ENFP, and ESFJ. This behavior results in suboptimal performance, suggesting a challenge in accurately differentiating between diverse personality types. Notably, all three models—random forest, SVM, and logistic regression—demonstrate a relative proficiency in classifying posts associated with EN-type personalities. An inference can be drawn from these observations: individuals with EN-type personalities may have a higher posting frequency compared to other personality types. Furthermore, their posts might possess distinct characteristics or features that are more easily discernible by the machine learning models, contributing to the models’ better performance in classifying EN-type posts."
  },
  {
    "objectID": "nlp.html#new-column-score_category",
    "href": "nlp.html#new-column-score_category",
    "title": "Milestone 2 NLP",
    "section": "1.1 New Column “score_category”",
    "text": "1.1 New Column “score_category”\nA new categorical column, “score_category,” was introduced to the comments dataset to categorize the original numerical ‘comment_score’. This stratification was informed by the calculated 25th, 50th, and 75th percentiles, ensuring an equitable division. Scores below the 25th percentile were classified as “Low,” those between the 25th and 50th percentiles as “Medium,” between the 50th and 75th as “High,” and scores above the 75th percentile as “Very High.” This new variable will be use in later analysis, to examine the relationship between comment scores and their sentiment labels.\n\n\nCode\n# Load data\ncomment_load = spark.read.parquet(f\"{workspace_wasbs_base_url}/mbti_comments.parquet\")\n# Cache the dataset\ncomment_load.cache()\n# Calculate the 25th, 50th, and 75th percentiles\nquantiles = comment_load.stat.approxQuantile(\"comment_score\", [0.25, 0.5, 0.75], 0.0)\n\nprint(f\"25th percentile: {quantiles[0]}\")\nprint(f\"50th percentile (median): {quantiles[1]}\")\nprint(f\"75th percentile: {quantiles[2]}\")\n\ncomment_score_summary = comment_load.describe(['comment_score'])\ncomment_score_summary.show()\n\n# Create a new categorical column based on comment_score division\ndef score_category(score):\n    if score <= quantiles[0]:\n        return 'Low'\n    elif score <= quantiles[1]:\n        return 'Medium'\n    elif score <= quantiles[2]:\n        return 'High'\n    else:\n        return 'Very High'\n\nscore_category_udf = F.udf(score_category)\n\ncomment_load = comment_load.withColumn(\"score_category\", score_category_udf(\"comment_score\"))\n\n# View the schema to confirm the new column addition\ncomment_load.printSchema()\n\n\n\n\n\nComment Score Summary Statistics \n\n\nsummary\ncomment_score\n\n\n\n\ncount\n1.83414e+06\n\n\nmean\n4.35266\n\n\nstddev\n13.5467\n\n\nmin\n-126\n\n\nmax\n1259\n\n\n\n\n\n\n\n\n\n\n\nQuantile value to divide comment score.\n\n\n\n\n\n\n\nUpdated Comment Data Column list."
  },
  {
    "objectID": "nlp.html#sentiment-analysis-using-pre-trained-model",
    "href": "nlp.html#sentiment-analysis-using-pre-trained-model",
    "title": "Milestone 2 NLP",
    "section": "1.2 Sentiment Analysis Using Pre-trained Model",
    "text": "1.2 Sentiment Analysis Using Pre-trained Model\n\n1.2.1 Sentiment Label Added\nUtilizing a pretrained model, we applied sentiment analysis to the text of each comment, assigning a sentiment label—positive, neutral, or negative—based on the comment’s content. This process effectively transformed the unstructured textual data into structured, categorical insights.\n\n\nCode\n# Define the name of the SentimentDLModel\nMODEL_NAME = \"sentimentdl_use_twitter\"  # Replace with the model name you intend to use\n\n# Configure the Document Assembler\ndocumentAssembler = DocumentAssembler()\\\n    .setInputCol(\"comment_text\")\\\n    .setOutputCol(\"document\")\n\n# Configure the Universal Sentence Encoder\nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n    .setInputCols([\"document\"])\\\n    .setOutputCol(\"sentence_embeddings\")\n\n# Configure the SentimentDLModel\nsentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n    .setInputCols([\"sentence_embeddings\"])\\\n    .setOutputCol(\"sentiment\")\n\n# Set up the NLP Pipeline\nnlpPipeline = Pipeline(\n    stages=[\n        documentAssembler,\n        use,\n        sentimentdl\n    ])\n\n# Apply the Pipeline to your DataFrame\npipelineModel = nlpPipeline.fit(comment_load)\nresults = pipelineModel.transform(comment_load)\n\nresult_df = results.select(\"comment_text\",\"comment_controversiality\",\"reply_to\",\"score_category\",F.explode(\"sentiment.result\").alias(\"sentiment\"))\nresult_df.show(10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncomment_text\ncomment_controversiality\nreply_to\nscore_category\nsentiment\n\n\n\n\nyes it feels like i’m finally understanding myself and knowing that i’m not the only one who feels this way🥰\n0\nt3\nVery High\npositive\n\n\nHahaha! What?????\n0\nt1\nLow\npositive\n\n\n[deleted]\n0\nt1\nMedium\nnegative\n\n\nI’d photo my friends through the window while they were asleep and put the photos in their notebooks.\n0\nt3\nLow\npositive\n\n\n\n\n\n\n\n1.2.2 Results\nOur initial analysis of sentiment in the MBTI subreddit discussions relied on raw count data, which suggested that ‘Low’ score category comments were predominantly negative, indicating a prevalence of critical voices. In contrast, ‘Medium’ and ‘Very High’ score categories seemed to have a higher share of positive comments, pointing to a more favorable reception of contributions in these categories. The ‘High’ score category appeared to have a balanced sentiment distribution, hinting at diverse engagement levels within the subreddit.\nHowever, this approach was flawed due to the uneven total number of comments across score categories. To correct this, we calculated the percentage of sentiments within each category, revealing a different picture: positive sentiment was actually dominant across all categories, with the ‘Low’ category at 62.88% positive, contrary to our initial findings. This percentage-based analysis helped clarify that, irrespective of score categories, there is a consistent trend of positive sentiment within the MBTI community discussions on Reddit.\n\n\n\nSentiment Labels Group by Count \n\n\nscore_category\nsentiment\ncount\npercentage\n\n\n\n\nLow\nneutral\n43229\n5.33\n\n\nLow\nnegative\n257820\n31.79\n\n\nMedium\npositive\n288381\n69.27\n\n\nVery High\npositive\n274252\n65.99\n\n\nLow\npositive\n509866\n62.88\n\n\nHigh\nnegative\n50260\n26.28\n\n\nMedium\nnegative\n105302\n25.29\n\n\nVery High\nnegative\n118009\n28.39\n\n\nMedium\nneutral\n22645\n5.44\n\n\nVery High\nneutral\n23356\n5.62\n\n\nHigh\nneutral\n10537\n5.51\n\n\nHigh\npositive\n130481\n68.22\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Reshape the data for heatmap plotting\nheatmap_data = df.pivot(index='score_category', columns='sentiment', values='count')\n\n#heatmap_data = df.pivot(\"score_category\", \"sentiment\",\"count\")\n# Convert the 'score_category' to a categorical type with the desired order\nordered_categories = ['Low','Medium','High','Very High']\nheatmap_data.index = pd.CategoricalIndex(heatmap_data.index, categories=ordered_categories, ordered=True)\n\n# Sort the DataFrame by the 'score_category' index to ensure the order is applied\nheatmap_data.sort_index(level='score_category', ascending=False, inplace=True)\nplt.figure(figsize=(12, 8))\nsentiment_heatmap = sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\nplt.title('Heatmap of Sentiment Counts by Score Category')\nplt.ylabel('Score Category')\nplt.xlabel('Sentiment')\n\n#plt.savefig('Users/ml2078/fall-2023-reddit-project-team-10/plots/csv/heatmap.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Reshape the data for heatmap plotting\nheatmap_data = df.pivot(index='score_category', columns='sentiment', values='percentage')\n\n#heatmap_data = df.pivot(\"score_category\", \"sentiment\",\"count\")\n# Convert the 'score_category' to a categorical type with the desired order\nordered_categories = ['Low','Medium','High','Very High']\nheatmap_data.index = pd.CategoricalIndex(heatmap_data.index, categories=ordered_categories, ordered=True)\n\n# Sort the DataFrame by the 'score_category' index to ensure the order is applied\nheatmap_data.sort_index(level='score_category', ascending=False, inplace=True)\nplt.figure(figsize=(12, 8))\nsentiment_heatmap = sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\nplt.title('Heatmap of Sentiment Percentages by Score Category')\nplt.ylabel('Score Category')\nplt.xlabel('Sentiment')\n\n#plt.savefig('Users/ml2078/fall-2023-reddit-project-team-10/plots/csv/heatmap.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "nlp.html#word-length-distribution",
    "href": "nlp.html#word-length-distribution",
    "title": "Milestone 2 NLP",
    "section": "2.1 Word Length Distribution",
    "text": "2.1 Word Length Distribution\nOur word length distribution data processing has been shown in the eda proposal 1.\n\n2.1.1 Word length distribution of the Submission Title\nAs the plot show, we can see that the distribution of the submission title length is right skewed, which means that most of the submission title length is short. And the distribution is also unimodal, which means that there is only one peak in the distribution. The peak is around 30 words, which means that most of the submission title length is around 30 words.\n\n\nCode\nimport random\ngroup_size = 10\nmax_length = 312  # Maximum length\n\n\nnum_groups = (max_length // group_size) + 1\n\n# Create a list of lists to store lengths in each group\ngrouped_data = [[] for _ in range(num_groups)]\n\n# Place the lengths into their respective groups\nfor len_val in title_lengths:\n    group_index = len_val // group_size\n    grouped_data[group_index].append(len_val)\n\n# Create a list to store the sampled data from each group\nsampled_data = []\n\n# Get a 10% random sample from each group\nfor group in grouped_data:\n    sample_size = max(1, int(1 * len(group)))  # Ensure at least 1 sample is taken\n    sampled_data.extend(random.sample(group, sample_size))\n\n# Create a distribution plot using Plotly\nfig = ff.create_distplot([sampled_data], ['Submission Title'], bin_size=5)\nfig.update_layout(\n    title='Submission Title Length Distribution',\n    xaxis_title='Length of Submission Title',\n    yaxis_title='Density'\n)\nfig.show()\n\n\n\n\n\nSubmission Title Length Distribution\n\n\n\n\n2.1.2 Comment length distribution\nAs for the comments length, T3 comments mean the direct comments to the submission and T1 comments mean the comments to the T3 comments. As the plot shows, we can see that the distribution of the T3 comments length is right skewed, which means that most of the T3 comments length is short. And the distribution is also unimodal, which means that there is only one peak in the distribution. The peak is around 10 words, which means that most of the T3 comments length is around 10 words. As for the T1 comments, the distribution is also right skewed and unimodal, but the peak is around 10 words, which means that most of the T1 comments length is around 10 words. And the distribution of T1 comments length is similar right skewed as the distribution of T3 comments length. We can infer that all the comments tend to be short.\n\n\nCode\ngroup_size = 10 # Group size\nmax_length = 999  # Maximum length\n\n# Calculate the number of groups\nnum_groups = (max_length // group_size) + 1\n\n# Create a list of lists to store lengths in each group\ngrouped_data = [[] for _ in range(num_groups)]\n\n# Place the lengths into their respective groups\nfor len_val in comment_t1_lengths:\n    group_index = min(len_val // group_size, num_groups - 1)  # Ensure the index doesn't exceed the range\n    grouped_data[group_index].append(len_val)\n\n# Create a list to store the sampled data from each group\nsampled_data = []\n\n# Get a 10% random sample from each group\nfor group in grouped_data:\n    if len(group) > 0:\n        sample_size = max(1, int(1 * len(group)))  # Ensure at least 1 sample is taken\n        sample_size = min(sample_size, len(group))  # Use the minimum of 10% sample or group size\n        sampled_data.extend(random.sample(group, sample_size))\n\n # Create a list of lists to store lengths in each group\ngrouped_data_t3 = [[] for _ in range(num_groups)]           \n# Place the lengths into their respective groups\nfor len_val in comment_t3_lengths:\n    group_index = min(len_val // group_size, num_groups - 1)  # Ensure the index doesn't exceed the range\n    grouped_data_t3[group_index].append(len_val)\n\n# Create a list to store the sampled data from each group\nsampled_data_t3 = []\n\n# Get a 10% random sample from each group\nfor group in grouped_data_t3:\n    if len(group) > 0:\n        sample_size = max(1, int(1 * len(group)))  # Ensure at least 1 sample is taken\n        sample_size = min(sample_size, len(group))  # Use the minimum of 10% sample or group size\n        sampled_data_t3.extend(random.sample(group, sample_size))\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Assuming sampled_data and sampled_data_t3 are your data arrays\n\n# Create subplots\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True,figsize=(8, 6))\nax1.hist(sampled_data, bins=50, color=(0.2, 0.7, 0.4, 0.7), alpha=0.7,density=True)\nsns.kdeplot(sampled_data, color=(0.2, 0.7, 0.4), ax=ax1)\nax1.set_title('T1 Comment Length Distribution Plot')\nax1.set_xlabel('Length')\nax1.set_ylabel('Frequency')\n\n# Plot histogram for Comment T3\nax2.hist(sampled_data_t3, bins=50, color=(0.5, 0, 0.5, 0.7), alpha=0.7,density=True)\nsns.kdeplot(sampled_data_t3, color=(0.5, 0, 0.5), ax=ax2)\nax2.set_title('T3 Comment Length Distribution Plot')\nax2.set_xlabel('Length')\nax2.set_ylabel('Frequency')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the combined plot\nplt.savefig(\"Users/xl659/fall-2023-reddit-project-team-10/data/plots/all_comments_length_distribution.png\")\nplt.show()\n\n\n\n\n\ncomment length distribution"
  },
  {
    "objectID": "nlp.html#the-most-common-words-in-the-submission-title",
    "href": "nlp.html#the-most-common-words-in-the-submission-title",
    "title": "Milestone 2 NLP",
    "section": "2.2 The most common words in the submission title",
    "text": "2.2 The most common words in the submission title\nIn order to understand the most common words that exist in the Reddit submissions related to MBTI, we could use the wordcloud to get the word frequency in all the submissions.\n\n\nCode\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\nfrom pyspark.ml.linalg import DenseVector\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom pyspark.ml.clustering import LDA\n\n# Step 1: Tokenization (if not done previously)\ntokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\ndf_tokenized = tokenizer.transform(df_cleaned)\n\n# Step 2: Remove Stopwords\nstopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\ndf_no_stopwords = stopwords_remover.transform(df_tokenized)\n\n# Step 3: Count Vectorization\ncount_vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\ncount_vectorizer_model = count_vectorizer.fit(df_no_stopwords)\ndf_count_vectorized = count_vectorizer_model.transform(df_no_stopwords)\n\n# Step 4: Term Frequency-Inverse Document Frequency (TF-IDF) transformation\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\nidf_model = idf.fit(df_count_vectorized)\ndf_tfidf = idf_model.transform(df_count_vectorized)\n\n# Step 5: Build the LDA model\nnum_topics = 10\nlda = LDA(k=num_topics, maxIter=30, featuresCol=\"features\")\n\n# Step 6: Create a pipeline\npipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer_model, idf_model, lda])\n\n\n\n\nCode\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndf_cleaned = pd.read_csv(\"../data/csv/cleaned_text.csv\")\ndf_cleaned[\"cleaned_text\"] = df_cleaned[\"cleaned_text\"].astype(str)\ntext = \" \".join(df_cleaned[\"cleaned_text\"])\n\n# Generate a WordCloud\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n\n# Display the WordCloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\n#plt.savefig(\"../data/plots/submission_wordcloud.png\")\nplt.show()\n\n\n\n\n\nFrom the wordcloud above, we can see that in the MBTI related submission titles, the most frequent words are “type”, “personality”, “mbti’. It is reasonable to have these words in MBTI related Reddit posts. Besides, the basic information of the MBTI types are also frequently mentioned in the titles, such as”intj”, “enfp”, “infj”, “entp”, “intp”, “enfj”, “istp”, “istj”, “entj”, “isfp”, “infp”, “estp”, “isfj”, “estj”, “esfp”, “esfj”.We may infer that Reddit users like to post submissions to ask what people think about their MBTI types and guess what the MBTI types of others are."
  },
  {
    "objectID": "nlp.html#important-words-with-tf-idf",
    "href": "nlp.html#important-words-with-tf-idf",
    "title": "Milestone 2 NLP",
    "section": "2.3 Important words with TF-IDF",
    "text": "2.3 Important words with TF-IDF\nTerm Frequency-Inverse Document Frequency (TF-IDF) is a crucial concept in natural language processing and information retrieval. It serves as a numerical statistic that reflects the significance of a term within a collection of documents. TF-IDF is calculated by combining two metrics: Term Frequency (TF), representing the frequency of a term within a specific document, and Inverse Document Frequency (IDF), measuring the rarity of the term across the entire document set. For each submission, the top 5 important words are selected from the tf-idf dataframe. We use the first 10 rows as an example. We can see that based on the top words in each row, type, mbti and think are important words in the submission.\n\n\n\n\n\n\n\n\n\n\nsubmission_title\ncleaned_text\ntop_words\n\n\n\n\nHelp me type my BF, pls!\nhelp me type my bf pls\n[‘type’, ‘help’, ‘pls’, ‘bf’]\n\n\nPerfectionism in Ti vs Te users\nperfectionism in ti vs te users\n[‘vs’, ‘ti’, ‘te’, ‘users’, ‘perfectionism’]\n\n\nWhich MBTI is most likely to judge someone for being cringe and conform to social norms and pressures?\nwhich mbti is most likely to judge someone for being cringe and conform to social norms and pressures\n[‘mbti’, ‘likely’, ‘someone’, ‘social’, ‘judge’]\n\n\nWould this be a function?\nwould this be a function\n[‘function’]\n\n\nis Ni possible without hunches\nis ni possible without hunches\n[‘ni’, ‘possible’, ‘without’, ‘hunches’]\n\n\nFound this visual to be accurate, what do you think?\nfound this visual to be accurate what do you think\n[‘think’, ‘accurate’, ‘found’, ‘visual’]\n\n\nCan underdeveloped inferior Si affect how dominant Ne manifests itself?\ncan underdeveloped inferior si affect how dominant ne manifests itself\n[‘ne’, ‘si’, ‘inferior’, ‘dominant’, ‘affect’]\n\n\nVoting\nvoting\n[‘voting’]\n\n\nMOST TO LEAST ATTRACTIVE TYPES (I’m a ISTP)\nmost to least attractive types im a istp\n[‘im’, ‘types’, ‘istp’, ‘least’, ‘attractive’]\n\n\nwhich mbti is the most likely to steal food from someone in a shared fridge?\nwhich mbti is the most likely to steal food from someone in a shared fridge\n[‘mbti’, ‘likely’, ‘someone’, ‘food’, ‘steal’]"
  },
  {
    "objectID": "nlp.html#topic-modeling-with-lda",
    "href": "nlp.html#topic-modeling-with-lda",
    "title": "Milestone 2 NLP",
    "section": "2.4 Topic Modeling with LDA",
    "text": "2.4 Topic Modeling with LDA\nLatent Dirichlet Allocation (LDA) is a generative probabilistic model used for topic modeling. Topic modeling is a technique in natural language processing (NLP) that aims to automatically identify topics present in a text corpus. LDA is an unsupervised machine learning approach; it doesn’t need any training data. All it needs is a document-word matrix as input. So in order to have a more concise understanding of the topics discussed in Reddit related to MBTI, we use LDA to build a topic model. The expectation results of the LDA model is seperate topics with specific related topic words in each topic. The topic words in each topic should be related to a same topic.\n\n\nCode\nfrom pyspark.ml.feature import CountVectorizer, IDF\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml import Pipeline\n#Fit the pipeline to the data\nlda_model = pipeline.fit(df_cleaned)\n\n# Step 8: Get the topics and associated terms\ntopics = lda_model.stages[-1].describeTopics()\n\n# Show the topics and associated terms\nprint(\"LDA Topics:\")\ntopics.show(truncate=False)\n\n# Step 9: Transform the original DataFrame to include topic distributions\ndf_lda_result = lda_model.transform(df_cleaned)\n\n# Show the LDA result DataFrame\nprint(\"LDA Result DataFrame:\")\ndf_lda_result.select(\"id\", \"cleaned_text\", \"filtered_words\", \"topicDistribution\").show(truncate=False)\nvocab_list = count_vectorizer_model.vocabulary\ntopic_list = []\nfor topic_row in topics.collect():\n    topic = topic_row.topic\n    indices = topic_row.termIndices\n    words = [vocab_list[idx] for idx in indices]\n    print(f\"Topic {topic}: {', '.join(words)}\")\n    topic_list.append( [', '.join(words)])\ntopics_df = topics.toPandas()\ntopics_df['topic_words']=topic_list\n\n\n\n\nCode\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport ast\n# read the topic data\ntopic_df = pd.read_csv(\"../data/csv/topic.csv\")\n# transfer the data into appropriate format\ntopic_df['termIndices'] = topic_df['termIndices'].apply(lambda x: [int(idx) for idx in x.strip('[]').split()])\ntopic_df['termWeights'] = topic_df['termWeights'].apply(lambda x: [float(weight) for weight in x.strip('[]').replace('\\n', '').split()])\ntopic_df['topic_words'] = topic_df['topic_words'].apply(lambda x: ast.literal_eval(x)[0].split(', '))\n\ncolor_list = ['#1f80b8', '#2498c1', '#37acc3', '#52bcc2', '#73c8bd', '#97d6b9', '#bde5b5', '#d6efb3', '#eaf7b1', '#f5fbc4']\n\n# Create subplots with a smaller vertical_spacing\nfig = make_subplots(rows=5, cols=2, subplot_titles=[f\"Topic {i}\" for i in range(10)], vertical_spacing=0.05)\n\n# Define a function to create a bar chart for each topic\ndef create_topic_plot(df, topic,color):\n    # Sort the weights in descending order while maintaining the association with the corresponding words\n    sorted_indices = sorted(range(len(df['termWeights'][topic])), key=lambda k: df['termWeights'][topic][k], reverse=False)\n    sorted_weights = [df['termWeights'][topic][i] for i in sorted_indices]\n    sorted_words = [df['topic_words'][topic][i] for i in sorted_indices]\n    \n    return go.Bar(\n        x=sorted_weights,\n        y=sorted_words,\n        orientation='h',\n        name=f'Topic {topic}',\n        marker_color=color  # Set the color of the bar\n    )\n\n# Add plots for each topic to the subplots\nfor topic in topic_df['topic']:\n    row = (topic // 2) + 1\n    col = (topic % 2) + 1\n    # Use the modulo operator to cycle through the color list\n    color = color_list[topic % len(color_list)]\n    fig.add_trace(create_topic_plot(topic_df, topic, color), row=row, col=col)\n\n# Update layout to make the gap between subplots smaller\nfig.update_layout(\n    title_text=\"LDA Topic Weights Plot using Plotly\",\n    title_x=0.5,  # This centers the title\n    height=1200,  # Adjusted for better spacing\n    showlegend=False,\n    margin=dict(l=20, r=20, b=20)  # Adjust margins to minimize white space\n)\n\n# Show the figure\nfig.show()\n\n\n\n                                                \n\n\nThe topics inferred from the LDA model reveal intriguing insights into the content of Reddit submissions related to MBTI. Each topic is characterized by a dominant theme, shedding light on the diverse discussions within the community.\n\nTopic 0: Users Seeking Common Ground\n\nDominant Word: “User”\nInference: The topic centers around Reddit users aiming for a shared understanding of MBTI types.\n\nTopic 1: Family Dynamics and MBTI\n\nDominant Theme: Family\nInference: Discussions delve into the relationships between different MBTI types and their families.\n\nTopic 2: Questioning the MBTI Universe\n\nDominant Theme: Questions\nInference: Topics revolve around a variety of questions related to MBTI.\n\nTopic 3: Personal MBTI Experiences\n\nDominant Theme: User MBTI Types\nInference: Submissions primarily focus on users sharing their personal MBTI experiences.\n\nTopic 4: Interpersonal Dynamics Between MBTI Types\n\nDominant Theme: Relationships\nInference: Conversations explore the dynamics between individuals with different MBTI types.\n\nTopic 5: Exploring Thoughts and Friendships\n\nDominant Theme: Thoughts\nInference: Topics touch upon the thoughts of different MBTI types and potentially delve into friendships between them.\n\nTopic 6: Speculating on MBTI Types\n\nDominant Theme: Guess\nInference: Discussions and speculations abound regarding guessing the MBTI types of individuals.\n\nTopic 7: Love Lives and Social Status Across MBTI Types\n\nDominant Themes: Love, Social Status\nInference: Conversations explore the realms of love lives and social statuses associated with different MBTI types.\n\nTopic 8: MBTI AMAs (Ask Me Anything)\n\nDominant Theme: AMA\nInference: Submissions where users inquire about anything related to a specific MBTI type.\n\nTopic 9: Unpacking Cognitive Functions (N, I, F, T, E)\n\nDominant Themes: N, I, F, T, E (Cognitive Functions)\nInference: Discussions revolve around understanding the cognitive functions associated with different MBTI types."
  },
  {
    "objectID": "nlp.html#vocabulary-richness-and-complexity-analysis",
    "href": "nlp.html#vocabulary-richness-and-complexity-analysis",
    "title": "Milestone 2 NLP",
    "section": "3.1 Vocabulary Richness and Complexity Analysis",
    "text": "3.1 Vocabulary Richness and Complexity Analysis\nIn our endeavor to unravel the linguistic intricacies within the MBTI community on Reddit, a key focus lies in the Vocabulary Richness and Complexity Analysis. This segment of our study is dedicated to quantitatively assessing the diversity and sophistication of language used by individuals of different MBTI types.\nWe aim to calculate and analyze various metrics for each post, including Lexical Density, which measures the proportion of unique words to the total words, and Lexical Variety, which evaluates the range of different words used. Additionally, the Average Word Length will be considered to gauge the complexity of vocabulary. To complement these metrics, readability indices such as the Gunning Fog Index and the Flesch-Kincaid Readability Tests will be employed. These tools will help in determining the level of education required to comprehend the texts and the ease with which they can be read.\n\n\nCode\nimport numpy as np \nimport pandas as pd \nimport os\nimport seaborn as sns\nfrom os import path\nfrom PIL import Image\nfrom collections import Counter \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n\n# Load the data \ndf_post = pd.read_csv('../data/csv/clean_post.csv')\n\n# split for different dichotomous axes\ndf_post['I_E'] = df_post['type'].str[0]\ndf_post['N_S'] = df_post['type'].str[1]\ndf_post['T_F'] = df_post['type'].str[2]\ndf_post['J_P'] = df_post['type'].str[3]\n\ndf_post['post'] = df_post['post'].astype(str)\ndf_post.head()\n\nimport textstat\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\n# Ensure you have the necessary NLTK data\nnltk.download('punkt')\n\ndef analyze_post(post):\n    # Tokenize the post and calculate lexical diversity and word length\n    tokens = word_tokenize(post)\n    num_tokens = len(tokens)\n    num_unique_tokens = len(set(tokens))\n    avg_word_length = sum(len(word) for word in tokens) / num_tokens if num_tokens > 0 else 0\n    \n    # Lexical diversity is the ratio of unique tokens to total tokens\n    lexical_diversity = num_unique_tokens / num_tokens if num_tokens > 0 else 0\n    \n    # Readability scores\n    flesch_reading_ease = textstat.flesch_reading_ease(post)\n    gunning_fog = textstat.gunning_fog(post)\n\n    return {\n        \"lexical_diversity\": lexical_diversity,\n        \"avg_word_length\": avg_word_length,\n        \"flesch_reading_ease\": flesch_reading_ease,\n        \"gunning_fog\": gunning_fog\n    }\n\n# Apply the analysis to each post\ndf_post['analysis'] = df_post['post'].apply(analyze_post)\n\n# Extracting each item in the 'analysis' into separate columns\ndf_features = pd.json_normalize(df_post['analysis'])\ndf_extended = pd.concat([df_post.drop('analysis', axis=1), df_features], axis=1)\n\ndf_extended.head()  \n\n\n\n\n\nVocabulary Richness and Complexity \n\n\n\n\n\n\n\n\n\n\ntype\npost\nlexical_diversity\navg_word_length\nflesch_reading_ease\ngunning_fog\n\n\n\n\nINFJ\nenfp and intj moments sportscenter not top ten plays pranks\n1\n5\n78.25\n8\n\n\nINFJ\nWhat has been the most life-changing experience in your life?\n1\n4.72727\n78.25\n8\n\n\nINFJ\nOn repeat for most of today.\n1\n3.28571\n90.77\n2.4\n\n\n\n\n\nAfter the processing for all the data, we now get the summary table for the analysis by grouping the types of MBTI.\n\n\nCode\n# Group by MBTI type and compute the average of each feature\ngrouped_analysis = df_posts.groupby('type').mean().reset_index()\ngrouped_analysis\n\n\n\n\n\n\n  \n    \n      \n      type\n      lexical_diversity\n      avg_word_length\n      flesch_reading_ease\n      gunning_fog\n    \n  \n  \n    \n      0\n      ENFJ\n      0.855082\n      3.715352\n      77.803870\n      7.655780\n    \n    \n      1\n      ENFP\n      0.856190\n      3.694274\n      79.522370\n      7.552258\n    \n    \n      2\n      ENTJ\n      0.863587\n      3.804960\n      76.428244\n      7.895928\n    \n    \n      3\n      ENTP\n      0.868430\n      3.779508\n      77.389784\n      7.824322\n    \n    \n      4\n      ESFJ\n      0.862479\n      3.732702\n      76.502893\n      7.570841\n    \n    \n      5\n      ESFP\n      0.868052\n      3.687037\n      80.307185\n      7.072569\n    \n    \n      6\n      ESTJ\n      0.859866\n      3.734023\n      79.480424\n      7.664124\n    \n    \n      7\n      ESTP\n      0.867671\n      3.719831\n      80.806471\n      7.258737\n    \n    \n      8\n      INFJ\n      0.856849\n      3.759766\n      77.563011\n      7.882373\n    \n    \n      9\n      INFP\n      0.856982\n      3.755605\n      78.675848\n      7.697734\n    \n    \n      10\n      INTJ\n      0.862162\n      3.809602\n      76.212240\n      8.033474\n    \n    \n      11\n      INTP\n      0.863211\n      3.815967\n      76.564830\n      8.028002\n    \n    \n      12\n      ISFJ\n      0.858493\n      3.721983\n      79.062925\n      7.497204\n    \n    \n      13\n      ISFP\n      0.859551\n      3.722106\n      79.845600\n      7.304264\n    \n    \n      14\n      ISTJ\n      0.860274\n      3.788507\n      77.761335\n      7.746899\n    \n    \n      15\n      ISTP\n      0.865300\n      3.728156\n      80.026725\n      7.383447\n    \n  \n\n\n\n\n\n3.1.1 Numerical Interpretation\n\nLexical Diversity:\nHigher lexical diversity implies a greater variety of vocabulary in the posts. The range is relatively narrow, indicating a fairly consistent use of diverse vocabulary across different MBTI types. Types like ENTP and ESFP show slightly higher diversity.\nAverage Word Length:\nLonger average word lengths can suggest a tendency to use more complex or formal language. Types like INTJ and INTP exhibit slightly longer average word lengths, potentially indicating a more complex language style.\nFlesch Reading Ease:\nThe Flesch Reading Ease score assesses text readability; higher scores indicate easier readability. Most MBTI types fall within a similar range, suggesting a general uniformity in readability. ESFP and ESTP types have higher scores, indicating their posts are slightly easier to read.\nGunning Fog Index:**\nThis index estimates the years of formal education needed to understand the text on the first reading. A range of 7 to 8 suggests the text is relatively straightforward, suitable for individuals with around 7 to 8 years of education. Types like INTJ and INTP have slightly higher scores, suggesting their posts may use slightly more complex language.\n\n\n\n3.1.2 Insights Summary\n\nMost posts, regardless of MBTI type, are written in a style that is relatively easy to read and understand.\nIntuitive types (N), such as INTJ and INTP, tend to use slightly longer words and a bit more complexity in their language use.\nThe Sensor types (S), such as ESFP and ESTP, show a tendency towards more practical and accessible language.\nIrrespective of specific type, generally communicates in a way that is diverse in vocabulary but still accessible, reflecting a balance between expressiveness and clarity."
  },
  {
    "objectID": "nlp.html#word-and-phrase-frequency-analysis",
    "href": "nlp.html#word-and-phrase-frequency-analysis",
    "title": "Milestone 2 NLP",
    "section": "3.2 Word and Phrase Frequency Analysis",
    "text": "3.2 Word and Phrase Frequency Analysis\nTo gain a more profound understanding of the communication styles prevalent among the MBTI community, our study incorporates a meticulous frequency analysis. This analysis is specifically designed to pinpoint the most frequently used words and phrases within the posts of each MBTI personality type.\n\n\nCode\n# remove the stopwords\nstopwords_list = set(STOPWORDS)\n# 'infj', 'entp', 'intp', 'intj', 'entj', 'enfj', 'infp', 'enfp', 'isfp', 'istp', 'isfj', 'istj', 'estp', 'esfp', 'estj', 'esfj', \nwords =['lot', 'time', 'love', 'actually', 'seem', 'need', 'infj', 'actually', 'pretty', 'sure', 'thought','type', 'one', 'even', 'someone', 'thing','make', \n            'now', 'see', 'things', 'feel', 'think', 'i', 'people', 'know', '-', \"much\", \"something\", \"will\", \"find\", \"go\", \"going\", \"need\", 'still', 'though', \n            'always', 'through', 'lot', 'time',  'really', 'want', 'way', 'never', 'find', 'say', 'it.', 'good', 'me.', 'many', 'first', 'wp', 'go', \n            'really', 'much', 'why', 'youtube', 'right', 'know', 'want', 'tumblr', 'great', 'say', 'well', 'people', 'will', 'something', 'way', 'sure', \n            'especially', 'thank', 'good', 'ye', 'person', 'https', 'watch', 'yes', 'got', 'take', 'person', 'life', 'might', 'me', 'me,', 'around', 'best', 'try', \n            'maybe', 'probability', 'usually', 'sometimes', 'trying', 'read', 'us', 'may', 'use', 'work', ':)', 'said', 'two', 'makes', 'little', 'quite', 'u', 'intps', 'probably', 'made', 'it', 'seems', 'look', 'yeah',\n           'different', 'come', 'it,', 'friends', 'entps', 'different', 'esfjs', 'look', 'infjs', 'estps', 'kind', 'intjs', 'enfjs', \n            'entjs', 'infps', 'every', 'long', 'tell', 'new', 'jpg','mean','year','thread']\n\nfor word in words:\n    stopwords_list.add(word)\n\nimport nltk\nfrom nltk.tokenize import word_tokenize, RegexpTokenizer\nfrom collections import Counter\nimport string\nfrom nltk.corpus import stopwords\n\n\n# Define a function to process text, remove stopwords, contractions, MBTI types, and count top 20 words\ndef process_text(posts, mbti_type):\n    stop_words = set(stopwords.words('english'))\n    tokenizer = RegexpTokenizer(r'\\b[a-zA-Z]+\\b')   # Tokenizer to remove punctuation\n\n    # Additional words to filter (MBTI types and common contractions)\n    additional_filters = set(['n\\'t', '\\'s', '\\'m', '\\'ve', '\\'re', '\\'ll', '\\'d'] + list(mbti_type))\n\n    # Tokenize and filter out stopwords and additional filters\n    words = [word for post in posts for word in tokenizer.tokenize(post.lower()) \n             if word not in stop_words and word not in stopwords_list and word not in additional_filters]\n    \n    # Count word frequency and keep only the top 20 words\n    word_freq = Counter(words).most_common(20)\n    \n    # Returning the top 20 words as a single string\n    return ', '.join([word for word, freq in word_freq])\n\n# Group by MBTI type and apply the function\ngrouped_word_freq = df_post.groupby('type').apply(lambda x: process_text(x['post'], x.name))\n\ngrouped_word_freq = grouped_word_freq.reset_index(name='top_words')\n\n\n\n\n\nSentiment Labels Group by Count \n\n\n\n\n\n\ntype\ntop_words\n\n\n\n\nENFJ\nenfj, lol, friend, thanks, infp, relationship, happy, others, back, everyone, help, post, better, fe, oh, agree, bit, haha, talk, anything\n\n\nENFP\nenfp, lol, friend, intj, thanks, enfps, oh, back, infp, definitely, guys, everyone, haha, p, happy, post, bit, anything, better, agree\n\n\nENTJ\nentj, intj, post, lol, point, anything, guys, types, understand, let, intp, back, better, others, everyone, mbti, agree, entp, thanks, personality\n\n\nENTP\nentp, intp, intj, ne, anything, enfp, lol, friend, point, post, oh, back, better, thinking, years, types, interesting, everyone, agree, understand\n\n\nESFJ\nesfj, fe, lol, intp, help, years, agree, happy, types, others, friend, thanks, definitely, anything, back, hard, mbti, personality, talking, infp\n\n\nESFP\nesfp, thanks, enfp, lol, intj, estp, anything, personality, mbti, better, back, post, guys, entp, isfp, entj, types, using, hard, laughing\n\n\nESTJ\nestj, infp, agree, enfp, friend, relationship, types, lol, estjs, dont, years, guy, personality, entj, anything, thanks, believe, point, day, guys\n\n\nESTP\nestp, lol, istp, friend, entp, fun, im, anything, guess, intj, back, let, intp, point, istj, esfp, se, thanks, guys, bad\n\n\nINFJ\nfriend, years, others, lol, infp, back, post, day, feeling, anything, better, world, hard, understand, thanks, intj, everyone, agree, mind, thinking\n\n\nINFP\ninfp, years, friend, world, back, day, feeling, anything, post, better, thanks, happy, everyone, hard, lol, school, oh, others, bit, bad\n\n\nINTJ\nintj, post, friend, anything, point, better, back, understand, years, world, others, mind, types, thinking, intp, agree, interesting, believe, question, give\n\n\nINTP\nintp, anything, intj, thinking, post, back, mind, point, better, years, world, understand, friend, believe, day, school, bit, guess, oh, interesting\n\n\nISFJ\nisfj, friend, definitely, others, lol, back, isfjs, help, si, post, agree, thanks, fe, types, infp, hard, better, school, bit, welcome\n\n\nISFP\nisfp, infp, lol, friend, thanks, anything, types, happy, music, back, years, hard, school, fi, better, feeling, guys, mbti, personality, guess\n\n\nISTJ\nistj, years, friend, back, anything, day, thanks, others, post, relationship, lol, thinking, types, last, better, school, happy, intj, guess, help\n\n\nISTP\nistp, anything, back, years, friend, day, better, istps, talk, thinking, school, give, stuff, point, lol, bit, types, mind, last, thanks\n\n\n\n\n\n\nCommon points:\nSocial relationships: The high-frequency words of most personality types include words indicating social relationships, such as “friend”, “relationship”, etc. This shows that on social media, regardless of MBTI, people generally tend to discuss relationships with relationships. Related topics, this may also be the meaning of this topic, to summarize and discuss the interpersonal relationships of different MBTIs.\nPositive emotions: Positive emotion words such as “happy” and “thanks” appear in many types of lists, which may reflect people’s tendency to share positive emotions and gratitude when discussing MBTI on social media.\n\n\nDifferences:\nPersonality-specific topics: Certain words seem to be more relevant to specific personality types. For example, INT types tend to use words such as “think” and “understand” that reflect introspection and logical analysis. Communication style: For example, Feeling types (e.g., ESFJ, ESFP) use words such as “lol” and “haha” that express humor or a light-hearted attitude, which may indicate that these types tend to be more informal and expressive in communication language.\nMBTI’s relationship with social media: The appearance of high-frequency words may reveal the behavior patterns of different personality types on social media. For example, intuitive individuals (N) may discuss more ideas and theories (such as “idea”, “theory”), while sensing individuals (S) may focus more on concrete and practical details."
  },
  {
    "objectID": "nlp.html#world-cloud-for-topic-interests",
    "href": "nlp.html#world-cloud-for-topic-interests",
    "title": "Milestone 2 NLP",
    "section": "3.3 World Cloud for Topic Interests",
    "text": "3.3 World Cloud for Topic Interests\n\n\nCode\nfrom wordcloud import WordCloud, STOPWORDS\n# remove the stopwords\nstopwords_list = set(STOPWORDS)\n# 'infj', 'entp', 'intp', 'intj', 'entj', 'enfj', 'infp', 'enfp', 'isfp', 'istp', 'isfj', 'istj', 'estp', 'esfp', 'estj', 'esfj', \nwords =['lot', 'time', 'love', 'actually', 'seem', 'need', 'infj', 'actually', 'pretty', 'sure', 'thought','type', 'one', 'even', 'someone', 'thing','make', \n            'now', 'see', 'things', 'feel', 'think', 'i', 'people', 'know', '-', \"much\", \"something\", \"will\", \"find\", \"go\", \"going\", \"need\", 'still', 'though', \n            'always', 'through', 'lot', 'time',  'really', 'want', 'way', 'never', 'find', 'say', 'it.', 'good', 'me.', 'many', 'first', 'wp', 'go', \n            'really', 'much', 'why', 'youtube', 'right', 'know', 'want', 'tumblr', 'great', 'say', 'well', 'people', 'will', 'something', 'way', 'sure', \n            'especially', 'thank', 'good', 'ye', 'person', 'https', 'watch', 'yes', 'got', 'take', 'person', 'life', 'might', 'me', 'me,', 'around', 'best', 'try', \n            'maybe', 'probability', 'usually', 'sometimes', 'trying', 'read', 'us', 'may', 'use', 'work', ':)', 'said', 'two', 'makes', 'little', 'quite', 'u', 'intps', 'probably', 'made', 'it', 'seems', 'look', 'yeah',\n           'different', 'come', 'it,', 'friends', 'entps', 'different', 'esfjs', 'look', 'infjs', 'estps', 'kind', 'intjs', 'enfjs', \n            'entjs', 'infps', 'every', 'long', 'tell', 'new', 'jpg','mean','year','thread']\n\nfor word in words:\n    stopwords_list.add(word)\n\n\n# Define list for dichotomous axes\nmbtiaxes_list = ['I_E', 'N_S', 'T_F', 'J_P']\ntypes_list = [['I','E'],['N','S'],['T','F'],['J','P']]\n\nfor n in range(4):\n    # Create a figure with 2 subplots\n    fig, axes = plt.subplots(1, 2, figsize=(36, 10)) # Two subplots side by side\n    sns.set_context('talk')\n\n    mbtiaxes = mbtiaxes_list[n]\n    types = types_list[n]\n\n    for m in range(2):\n        text_I = \"\".join(str(i) for i in df_posts[df_posts[mbtiaxes]== types[m]].post)\n        text_I = text_I.lower()\n        wordcloud_I = WordCloud(background_color='white', width=800, height=400, stopwords=stopwords_list, max_words=100, repeat=False, min_word_length=4).generate(text_I)\n        axes[m].imshow(wordcloud_I, interpolation='bilinear')\n        axes[m].axis('off')\n        axes[m].set_title('Most common tokenized words for ' + types[m], fontsize=25)\n\n        # Save the entire figure\n        #plt.savefig('mbti_token_clouds.png')\n\n    # Display the plot\n    plt.show()\n\n\n   \nI-E (Introversion vs. Extraversion): - Common: Both highlight “post” and “friend,” meaning that people regardless of whether they are introverts or extroverts value sharing and relationships on social media. - Difference: Extraverted types may use “lol” and “thanks” more, which suggests that extroverts may be more active on social media and tend to use more words that indicate positive emotions and social interactions.\nN-S (Intuition vs. Sensing): - Common: Both focus on “feel” and “think,” indicating that both intuitive and sensing types express their thoughts and emotions on social media. - Difference: Intuitive types are more likely to use “idea” and “understand,” which reflects their tendency to discuss concepts and understand deeper meanings, while sensing types are more likely to use concrete, everyday words such as “school” and “work.”\nT-F (Thinking vs. Feeling): - Common: Both use “friend” and “relationship”, showing that both thinking and feeling types value interpersonal relationships on social media. - Difference: Feeling types may use “happy” and “feel” more, emphasizing emotion and interpersonal harmony, while Thinking types may use more “question” and “point,” indicating that they focus more on logic and analysis on social media .\nJ-P (Judging vs. Perceiving): - Common: Both use “post” and “think” frequently, indicating that people with both judging and perceiving types will share their thoughts on social media. - Difference: Judging types may be more inclined to use “help” and “plan”, which may be related to their pursuit of organization and structure; while perceiving types may be more inclined to use “guess” and “question”, showing that they are more open and open-minded. Flexible attitude.\nIn summary, both ends of each personality dimension have unique communication patterns and concerns, but there are also some common social media behaviors. These analyzes can help us better understand how different individuals express themselves and interact in digital spaces."
  },
  {
    "objectID": "conclusion.html#project-summary",
    "href": "conclusion.html#project-summary",
    "title": "Conclusion",
    "section": "Project summary",
    "text": "Project summary\n\n1.MBTI Mirror: Community Engagement Disposition Mapping (EDA)\nThrough in-depth exploratory data analysis of post and comment data from the MBTI subreddit community, we discovered significant engagement changes and community activity trends. The analysis shows engagement patterns across different MBTI types, revealing that certain types of posts and comments are more frequent than others. This finding provides valuable insight into the distribution and interactions of personality types within communities.\n\n\nCode\n# load the preprocessed data\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport kaleido\n\ndf = pd.read_csv(\"../data/csv/time-series.csv\")\n\n# Create subplots\nfig = make_subplots(rows=2, cols=1, subplot_titles=(\"Monthly Submission Counts\", \"Average Comments and Scores\"))\n\n# Line plot \nfig.add_trace(\n    go.Scatter(x=df['submission_year'].astype(str) + '-' + df['submission_month'].astype(str), \n               y=df['count(submission_title)'], \n               name='Submission Counts', \n               mode='lines+markers'),\n    row=1, col=1\n)\n\n# Bar plot\nfig.add_trace(\n    go.Bar(x=df['submission_year'].astype(str) + '-' + df['submission_month'].astype(str), \n           y=df['avg_num_comments'], \n           name='Average Number of Comments'),\n    row=2, col=1\n)\n# Scatter plot \nfig.add_trace(\n    go.Scatter(x=df['submission_year'].astype(str) + '-' + df['submission_month'].astype(str), \n               y=df['avg_score'], \n               name='Average Score', \n               mode='markers'),\n    row=2, col=1\n)\n\n# Update xaxis\nfig.update_xaxes(title_text=\"Date\", row=1, col=1)\nfig.update_xaxes(title_text=\"Date\", row=2, col=1)\n\n# Update yaxis \nfig.update_yaxes(title_text=\"Submission Counts\", row=1, col=1)\nfig.update_yaxes(title_text=\"Average Values\", row=2, col=1)\n\n# Update title and layout\nfig.update_layout(\n    title_text=\"Reddit MBTI Topic Submission Trends\",\n    title_x=0.45,  # This centers the title\n    showlegend=True,\n\n)\nfig.update_layout(\n    title=dict(\n        font=dict(\n            size=20, # Adjusting the font size\n        )\n    )\n)\n\n# Show the figure\nfig.show()\n\n\n\n                                                \n\n\n\n\n2.MBTI Context: Exploring the discourse world of introversion and extroversion (NLP)\nUsing NLP technology to analyze the discussion content in the community, we successfully identified the main discussion topics and key words about introversion and extroversion. This not only reveals common perceptions of these character traits among community members, but also demonstrates differences in emotional tendencies and word choice. These insights provide a deeper understanding of the nuances of language expression across MBTI types.\n   \n\n\n3.MBTI Decoded: The Power of Predicting Personality with Machine Learning (ML)\nBy building and applying machine learning models, we successfully predicted MBTI types, demonstrating the model’s remarkable performance in accuracy and performance. Additionally, our analysis explores potential links between MBTI types and health data, revealing specific health risks and patterns that different personality types may face. These insights are extremely important for designing targeted health intervention strategies.\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objs as go\nimport networkx as nx\n\nrules_pd=pd.read_csv(\"../data/csv/ordered_association_rules.csv\")\n\n# Create a graph\nG = nx.DiGraph()\n\n# Add nodes and edges\nfor _, row in rules_pd.iterrows():\n    G.add_edge(str(row['antecedent']), str(row['consequent']), weight=row['confidence'])\n\n# Generate position layout\npos = nx.spring_layout(G)\n\n# Create edge trace\nedge_x = []\nedge_y = []\nfor edge in G.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.extend([x0, x1, None])\n    edge_y.extend([y0, y1, None])\n\nedge_trace = go.Scatter(\n    x=edge_x, y=edge_y,\n    line=dict(width=0.5, color='#888'),\n    hoverinfo='none',\n    mode='lines')\n\n# Create node trace\nnode_x = []\nnode_y = []\nfor node in G.nodes():\n    x, y = pos[node]\n    node_x.append(x)\n    node_y.append(y)\n\n#color_list=['#ffffd9', '#f5fbc4', '#eaf7b1', '#d6efb3', '#bde5b5', '#97d6b9', '#73c8bd', '#52bcc2', '#37acc3', '#2498c1', '#1f80b8', '#2165ab', '#234da0', '#253795', '#172978', '#081d58']\ncolor_list=['#081d58','#253795','#1f80b8','#97d6b9','#ffffd9']\n\nnode_trace = go.Scatter(\n    x=node_x, y=node_y,\n    mode='markers+text',  # Add 'text' to the mode\n    text=[node for node in G.nodes()],  # Add node labels\n    textposition=\"bottom center\",  # Position of text\n    hoverinfo='text',\n    marker=dict(\n        showscale=True,\n        colorscale=color_list,\n        reversescale=True,\n        color=[],\n        size=15,  # Increase node size\n        colorbar=dict(\n            thickness=15,\n            title='Number of Node Connections',\n            xanchor='left',\n            titleside='right'\n        ),\n        line_width=1.5))\n\n# Add node text and hover info\nnode_adjacencies = []\nnode_text = []\nfor node, adjacencies in enumerate(G.adjacency()):\n    node_adjacencies.append(len(adjacencies[1]))\n    node_text.append(f'{adjacencies[0]}')\n\nnode_trace.marker.color = node_adjacencies\nnode_trace.text = node_text\n\n# Create figure\nfig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title='<br>Network graph of association rules',\n                titlefont_size=23,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=80),\n                annotations=[dict(\n                    text=\"Python plotly library\",\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002)],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\n\nfig.show()\n\n\n\n                                                \n\n\nThrough an in-depth analysis of the MBTI subreddit community, this project successfully achieved insights into community engagement, MBTI type-specific discussions, trends over time, posting behavior within different personality dyads, and correlations with health data. Comprehensive understanding. This project also sheds light on how personality types shape patterns of communication and behavior in a community, providing a unique insight into this complex and diverse community."
  },
  {
    "objectID": "conclusion.html#next-steps-plan",
    "href": "conclusion.html#next-steps-plan",
    "title": "Conclusion",
    "section": "Next steps plan",
    "text": "Next steps plan\nTo further deepen our understanding of the MBTI subreddit community and improve the comprehensiveness of our analysis, our future plans will include several key directions. - First, we plan to examine comments and posts that may have been deleted or removed to ensure our data set is as complete as possible to provide a more accurate analysis of community engagement. - Second, we will set out to collect more MBTI-related health data to make broader generalizations about the relationship between different personality types and health conditions. This will not only enhance our current health-related analyses, but may also reveal new interesting associations. - Finally, we plan to increase interaction with users and audiences. By participating more actively in the community, we can better understand the needs and interests of our users, thereby further improving our analysis methods and research directions."
  },
  {
    "objectID": "conclusion.html#what-we-learned-from-this-project",
    "href": "conclusion.html#what-we-learned-from-this-project",
    "title": "Conclusion",
    "section": "What we learned from this project",
    "text": "What we learned from this project"
  }
]
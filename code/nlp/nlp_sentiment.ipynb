{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Sentiment Analysis"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "13",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:27:50.4357401Z",
              "session_start_time": "2023-11-13T23:27:50.5054256Z",
              "execution_start_time": "2023-11-13T23:28:42.5270173Z",
              "execution_finish_time": "2023-11-13T23:28:42.8996503Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "1c95e7ad-5da6-4653-8a8f-b1065036f83f"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 13, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f852455d7f0>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-a8007257:46023\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.2.5.1-100879434</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1699918123232
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f \\\n",
        "{\"conf\": {\"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.2\"}}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:40:03.7552713Z",
              "session_start_time": "2023-11-13T23:40:03.8967096Z",
              "execution_start_time": "2023-11-13T23:41:31.5269292Z",
              "execution_finish_time": "2023-11-13T23:41:31.5590875Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "fe7e8aec-9a1b-4014-bbb8-fc9079054413"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Unrecognized options: "
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:40:07.0952149Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:41:51.8639396Z",
              "execution_finish_time": "2023-11-13T23:41:59.9124499Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "7d906f70-df1d-407b-a374-603df72499ba"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting spark-nlp\n  Downloading spark_nlp-5.1.4-py2.py3-none-any.whl (540 kB)\n\u001b[K     |████████████████████████████████| 540 kB 9.3 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: spark-nlp\nSuccessfully installed spark-nlp-5.1.4\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace_default_storage_account = \"group10astorage46582e02e\"\n",
        "workspace_default_container = \"azureml-blobstore-e8a18b52-3288-4d1f-9f32-d5a9249c2c0e\"\n",
        "workspace_wasbs_base_url = (f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\")\n",
        "comment_load = spark.read.parquet(f\"{workspace_wasbs_base_url}/mbti_comments.parquet\")\n",
        "comment_load.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:40:10.4610458Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:42:00.0913898Z",
              "execution_finish_time": "2023-11-13T23:42:09.9649511Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 7:\nworkspace_default_storage_account = \"group10astorage46582e02e\"\nworkspace_default_container = \"azureml-blobstore-e8a18b52-3288-4d1f-9f32-d5a9249c2c0e\"\nworkspace_wasbs_base_url = (f\"wasbs://{workspace_default_container}@{workspace_default_storage_account}.blob.core.windows.net/\")\ncomment_load = spark.read.parquet(f\"{workspace_wasbs_base_url}/mbti_comments.parquet\")\ncomment_load.printSchema()",
                    "submissionTime": "2023-11-13T23:42:02.991GMT",
                    "completionTime": "2023-11-13T23:42:06.337GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "7",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "14625319-ce23-4b1e-9c10-240c190f19e4"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root\n |-- sub_id: string (nullable = true)\n |-- comment_author: string (nullable = true)\n |-- comment_text: string (nullable = true)\n |-- link_id: string (nullable = true)\n |-- comment_score: long (nullable = true)\n |-- comment_controversiality: long (nullable = true)\n |-- reply_to: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699918930152
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the dataset\n",
        "comment_load.cache()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:42:23.4418596Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:42:23.5958944Z",
              "execution_finish_time": "2023-11-13T23:42:24.3927233Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "390690cb-9691-4fbe-8c35-e61699f40de8"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "DataFrame[sub_id: string, comment_author: string, comment_text: string, link_id: string, comment_score: bigint, comment_controversiality: bigint, reply_to: string, year: int, month: int]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699918944671
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Find the range of comment_score\n",
        "min_score, max_score = comment_load.agg(\n",
        "    F.min(\"comment_score\"), \n",
        "    F.max(\"comment_score\")\n",
        ").first()\n",
        "\n",
        "print(f\"Range of comment_score: {min_score} to {max_score}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:48:27.8223144Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:48:27.9561761Z",
              "execution_finish_time": "2023-11-13T22:49:46.7130288Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "first at /tmp/ipykernel_8189/236520377.py:3",
                    "dataWritten": 0,
                    "dataRead": 14910,
                    "rowCount": 233,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "first at /tmp/ipykernel_8189/236520377.py:3",
                    "description": "Job group for statement 10:\nfrom pyspark.sql import functions as F\n# Find the range of comment_score\nmin_score, max_score = comment_load.agg(\n    F.min(\"comment_score\"), \n    F.max(\"comment_score\")\n).first()\n\nprint(f\"Range of comment_score: {min_score} to {max_score}\")",
                    "submissionTime": "2023-11-13T22:49:45.478GMT",
                    "completionTime": "2023-11-13T22:49:45.910GMT",
                    "stageIds": [
                      6,
                      7
                    ],
                    "jobGroup": "10",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 233,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at /tmp/ipykernel_8189/236520377.py:3",
                    "dataWritten": 14910,
                    "dataRead": 284164932,
                    "rowCount": 1834373,
                    "usageDescription": "",
                    "jobId": 5,
                    "name": "first at /tmp/ipykernel_8189/236520377.py:3",
                    "description": "Job group for statement 10:\nfrom pyspark.sql import functions as F\n# Find the range of comment_score\nmin_score, max_score = comment_load.agg(\n    F.min(\"comment_score\"), \n    F.max(\"comment_score\")\n).first()\n\nprint(f\"Range of comment_score: {min_score} to {max_score}\")",
                    "submissionTime": "2023-11-13T22:48:29.318GMT",
                    "completionTime": "2023-11-13T22:49:45.362GMT",
                    "stageIds": [
                      5
                    ],
                    "jobGroup": "10",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "398e858c-be4d-4ae3-be23-80f68f69efe1"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Range of comment_score: -126 to 1259\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699915786940
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the 25th, 50th, and 75th percentiles\n",
        "quantiles = comment_load.stat.approxQuantile(\"comment_score\", [0.25, 0.5, 0.75], 0.0)\n",
        "\n",
        "print(f\"25th percentile: {quantiles[0]}\")\n",
        "print(f\"50th percentile (median): {quantiles[1]}\")\n",
        "print(f\"75th percentile: {quantiles[2]}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:42:32.2729853Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:42:32.4090522Z",
              "execution_finish_time": "2023-11-13T23:43:55.387673Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "approxQuantile at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 305218,
                    "dataRead": 284470150,
                    "rowCount": 1834606,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "approxQuantile at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\n# Calculate the 25th, 50th, and 75th percentiles\nquantiles = comment_load.stat.approxQuantile(\"comment_score\", [0.25, 0.5, 0.75], 0.0)\n\nprint(f\"25th percentile: {quantiles[0]}\")\nprint(f\"50th percentile (median): {quantiles[1]}\")\nprint(f\"75th percentile: {quantiles[2]}\")",
                    "submissionTime": "2023-11-13T23:42:33.292GMT",
                    "completionTime": "2023-11-13T23:43:54.488GMT",
                    "stageIds": [
                      1,
                      2
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 247,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 247,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 247,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d07bbe16-13ff-4a41-a6eb-4bde6ca86bfd"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "25th percentile: 1.0\n50th percentile (median): 2.0\n75th percentile: 3.0\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699919035638
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comment_score_summary = comment_load.describe(['comment_score'])\n",
        "comment_score_summary.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:50:13.7499989Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:50:13.8911942Z",
              "execution_finish_time": "2023-11-13T22:50:16.2841872Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "describe at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 23200,
                    "rowCount": 233,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "describe at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 12:\ncomment_score_summary = comment_load.describe(['comment_score'])\ncomment_score_summary.show()",
                    "submissionTime": "2023-11-13T22:50:14.884GMT",
                    "completionTime": "2023-11-13T22:50:15.015GMT",
                    "stageIds": [
                      12,
                      11
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 233,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "describe at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 23200,
                    "dataRead": 333737919,
                    "rowCount": 476,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "describe at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 12:\ncomment_score_summary = comment_load.describe(['comment_score'])\ncomment_score_summary.show()",
                    "submissionTime": "2023-11-13T22:50:14.002GMT",
                    "completionTime": "2023-11-13T22:50:14.762GMT",
                    "stageIds": [
                      10
                    ],
                    "jobGroup": "12",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "b6eb4079-154e-4b22-b674-0bb5c5934f18"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+-------+------------------+\n|summary|     comment_score|\n+-------+------------------+\n|  count|           1834140|\n|   mean| 4.352661192711571|\n| stddev|13.546681999290229|\n|    min|              -126|\n|    max|              1259|\n+-------+------------------+\n\n"
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699915816527
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comment_score_summary.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_summary.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:53:51.3423499Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:53:51.4770138Z",
              "execution_finish_time": "2023-11-13T22:53:58.5695545Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_8189/3094922885.py:1",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "toPandas at /tmp/ipykernel_8189/3094922885.py:1",
                    "description": "Job group for statement 13:\ncomment_score_summary.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_summary.csv\",index=False)\n",
                    "submissionTime": "2023-11-13T22:53:55.947GMT",
                    "completionTime": "2023-11-13T22:53:56.543GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 5,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 5,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 5,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "e8c8528c-f04d-4768-b993-08006a693475"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 13, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699916038961
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Create a new categorical column based on comment_score division\n",
        "def score_category(score):\n",
        "    if score <= quantiles[0]:\n",
        "        return 'Low'\n",
        "    elif score <= quantiles[1]:\n",
        "        return 'Medium'\n",
        "    elif score <= quantiles[2]:\n",
        "        return 'High'\n",
        "    else:\n",
        "        return 'Very High'\n",
        "\n",
        "score_category_udf = F.udf(score_category)\n",
        "\n",
        "comment_load = comment_load.withColumn(\"score_category\", score_category_udf(\"comment_score\"))\n",
        "\n",
        "# View the schema to confirm the new column addition\n",
        "comment_load.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:42:36.4772564Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:43:55.5712638Z",
              "execution_finish_time": "2023-11-13T23:43:55.897891Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "e3b4383d-dd5c-47a6-9c1c-87abe7d08703"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root\n |-- sub_id: string (nullable = true)\n |-- comment_author: string (nullable = true)\n |-- comment_text: string (nullable = true)\n |-- link_id: string (nullable = true)\n |-- comment_score: long (nullable = true)\n |-- comment_controversiality: long (nullable = true)\n |-- reply_to: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- score_category: string (nullable = true)\n\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699919036165
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Assuming your DataFrame with the score_category column is named 'results'\n",
        "category_counts = comment_load.groupBy(\"score_category\",\"reply_to\").count()\n",
        "\n",
        "# Show the results\n",
        "category_counts.orderBy(\"score_category\",\"reply_to\").show()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:54:16.429045Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:54:16.5637804Z",
              "execution_finish_time": "2023-11-13T22:54:27.4772107Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 167808,
                    "rowCount": 1864,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 15:\nfrom pyspark.sql import functions as F\n\n# Assuming your DataFrame with the score_category column is named 'results'\ncategory_counts = comment_load.groupBy(\"score_category\",\"reply_to\").count()\n\n# Show the results\ncategory_counts.orderBy(\"score_category\",\"reply_to\").show()\n",
                    "submissionTime": "2023-11-13T22:54:26.772GMT",
                    "completionTime": "2023-11-13T22:54:27.115GMT",
                    "stageIds": [
                      15,
                      16
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 233,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 167808,
                    "dataRead": 333762496,
                    "rowCount": 2107,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 15:\nfrom pyspark.sql import functions as F\n\n# Assuming your DataFrame with the score_category column is named 'results'\ncategory_counts = comment_load.groupBy(\"score_category\",\"reply_to\").count()\n\n# Show the results\ncategory_counts.orderBy(\"score_category\",\"reply_to\").show()\n",
                    "submissionTime": "2023-11-13T22:54:16.841GMT",
                    "completionTime": "2023-11-13T22:54:26.339GMT",
                    "stageIds": [
                      14
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4d29db6c-723c-4949-bd10-409267135932"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------+------+\n|score_category|reply_to| count|\n+--------------+--------+------+\n|          High|      t1|101867|\n|          High|      t3| 89412|\n|           Low|      t1|430248|\n|           Low|      t3|380668|\n|        Medium|      t1|236557|\n|        Medium|      t3|179771|\n|     Very High|      t1|206793|\n|     Very High|      t3|208824|\n+--------------+--------+------+\n\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699916067704
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculate the total counts for each reply_to category\n",
        "total_counts = category_counts.groupBy(\"reply_to\").agg(F.sum(\"count\").alias(\"total_count\"))\n",
        "\n",
        "# Join the total counts back to the original DataFrame\n",
        "combined_data = category_counts.join(total_counts, \"reply_to\")\n",
        "\n",
        "# Calculate the percentage\n",
        "combined_data_with_percentage = combined_data.withColumn(\n",
        "    \"percentage\",\n",
        "    F.round((F.col(\"count\") / F.col(\"total_count\")),2),\n",
        ").orderBy(\"reply_to\",\"score_category\").select(\"reply_to\",\"score_category\",\"count\",\"percentage\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:54:21.8966059Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:54:27.6814271Z",
              "execution_finish_time": "2023-11-13T22:54:27.9868869Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "f71960c1-e762-42d4-98d2-4e660bf8b976"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 16, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699916068316
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data_with_percentage.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_percentage.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:55:08.2798707Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:55:08.4449475Z",
              "execution_finish_time": "2023-11-13T22:55:16.4094924Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 6,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "dataWritten": 0,
                    "dataRead": 786,
                    "rowCount": 8,
                    "usageDescription": "",
                    "jobId": 24,
                    "name": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "description": "Job group for statement 18:\ncombined_data_with_percentage.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_percentage.csv\",index=False)",
                    "submissionTime": "2023-11-13T22:55:15.344GMT",
                    "completionTime": "2023-11-13T22:55:15.522GMT",
                    "stageIds": [
                      38,
                      39,
                      40,
                      41
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 236,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 235,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 3,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "dataWritten": 786,
                    "dataRead": 146,
                    "rowCount": 10,
                    "usageDescription": "",
                    "jobId": 23,
                    "name": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "description": "Job group for statement 18:\ncombined_data_with_percentage.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_percentage.csv\",index=False)",
                    "submissionTime": "2023-11-13T22:55:15.192GMT",
                    "completionTime": "2023-11-13T22:55:15.253GMT",
                    "stageIds": [
                      37,
                      35,
                      36
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 235,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 234,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "dataWritten": 0,
                    "dataRead": 146,
                    "rowCount": 2,
                    "usageDescription": "",
                    "jobId": 22,
                    "name": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "description": "Job group for statement 18:\ncombined_data_with_percentage.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_percentage.csv\",index=False)",
                    "submissionTime": "2023-11-13T22:55:15.111GMT",
                    "completionTime": "2023-11-13T22:55:15.174GMT",
                    "stageIds": [
                      33,
                      34,
                      32
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 235,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 234,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "dataWritten": 146,
                    "dataRead": 167808,
                    "rowCount": 1866,
                    "usageDescription": "",
                    "jobId": 21,
                    "name": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "description": "Job group for statement 18:\ncombined_data_with_percentage.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_percentage.csv\",index=False)",
                    "submissionTime": "2023-11-13T22:55:14.880GMT",
                    "completionTime": "2023-11-13T22:55:14.992GMT",
                    "stageIds": [
                      30,
                      31
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 233,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "dataWritten": 167808,
                    "dataRead": 333737919,
                    "rowCount": 2107,
                    "usageDescription": "",
                    "jobId": 19,
                    "name": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "description": "Job group for statement 18:\ncombined_data_with_percentage.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_percentage.csv\",index=False)",
                    "submissionTime": "2023-11-13T22:55:08.697GMT",
                    "completionTime": "2023-11-13T22:55:14.818GMT",
                    "stageIds": [
                      27
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "dataWritten": 167808,
                    "dataRead": 333232751,
                    "rowCount": 2107,
                    "usageDescription": "",
                    "jobId": 18,
                    "name": "toPandas at /tmp/ipykernel_8189/355299175.py:1",
                    "description": "Job group for statement 18:\ncombined_data_with_percentage.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/comment_score_percentage.csv\",index=False)",
                    "submissionTime": "2023-11-13T22:55:08.639GMT",
                    "completionTime": "2023-11-13T22:55:11.779GMT",
                    "stageIds": [
                      26
                    ],
                    "jobGroup": "18",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "27e3f895-d29d-43d7-aa6b-538db85bd2d5"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 18, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699916117068
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results\n",
        "combined_data_with_percentage.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:54:34.4522501Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:54:34.6319119Z",
              "execution_finish_time": "2023-11-13T22:54:44.6979502Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 4,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 146,
                    "rowCount": 2,
                    "usageDescription": "",
                    "jobId": 17,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\n# Show the results\ncombined_data_with_percentage.show()",
                    "submissionTime": "2023-11-13T22:54:42.368GMT",
                    "completionTime": "2023-11-13T22:54:42.657GMT",
                    "stageIds": [
                      24,
                      25,
                      23
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 235,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 234,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 2,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 146,
                    "dataRead": 167808,
                    "rowCount": 1866,
                    "usageDescription": "",
                    "jobId": 16,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\n# Show the results\ncombined_data_with_percentage.show()",
                    "submissionTime": "2023-11-13T22:54:41.929GMT",
                    "completionTime": "2023-11-13T22:54:42.271GMT",
                    "stageIds": [
                      21,
                      22
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 233,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 167808,
                    "dataRead": 331658901,
                    "rowCount": 2107,
                    "usageDescription": "",
                    "jobId": 14,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\n# Show the results\ncombined_data_with_percentage.show()",
                    "submissionTime": "2023-11-13T22:54:35.165GMT",
                    "completionTime": "2023-11-13T22:54:41.816GMT",
                    "stageIds": [
                      18
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 167808,
                    "dataRead": 333166124,
                    "rowCount": 2107,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\n# Show the results\ncombined_data_with_percentage.show()",
                    "submissionTime": "2023-11-13T22:54:35.014GMT",
                    "completionTime": "2023-11-13T22:54:38.580GMT",
                    "stageIds": [
                      17
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d22662d3-65a7-4c21-ac0b-a7459fdc7f73"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------+--------------+------+----------+\n|reply_to|score_category| count|percentage|\n+--------+--------------+------+----------+\n|      t1|          High|101867|       0.1|\n|      t1|           Low|430248|      0.44|\n|      t1|        Medium|236557|      0.24|\n|      t1|     Very High|206793|      0.21|\n|      t3|          High| 89412|       0.1|\n|      t3|           Low|380668|      0.44|\n|      t3|        Medium|179771|      0.21|\n|      t3|     Very High|208824|      0.24|\n+--------+--------------+------+----------+\n\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699916084927
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import UniversalSentenceEncoder, SentimentDLModel\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Define the name of the SentimentDLModel\n",
        "MODEL_NAME = \"sentimentdl_use_twitter\"  # Replace with the model name you intend to use\n",
        "\n",
        "# Configure the Document Assembler\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"comment_text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Configure the Universal Sentence Encoder\n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "# Configure the SentimentDLModel\n",
        "sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "# Set up the NLP Pipeline\n",
        "nlpPipeline = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler,\n",
        "        use,\n",
        "        sentimentdl\n",
        "    ])\n",
        "\n",
        "# Apply the Pipeline to your DataFrame\n",
        "pipelineModel = nlpPipeline.fit(comment_load)\n",
        "results = pipelineModel.transform(comment_load)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:44:17.5725388Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:44:17.7043075Z",
              "execution_finish_time": "2023-11-13T23:45:31.9525179Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 4,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "first at Feature.scala:182",
                    "dataWritten": 0,
                    "dataRead": 650,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 5,
                    "name": "first at Feature.scala:182",
                    "description": "Job group for statement 11:\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder, SentimentDLModel\nfrom pyspark.ml import Pipeline\n\n# Define the name of the SentimentDLModel\nMODEL_NAME = \"sentimentdl_use_twitter\"  # Replace with the model name you intend to use\n\n# Configure the Document Assembler\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"comment_text\")    .setOutputCol(\"document\")\n\n# Configure the Universal Sentence Encoder\nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")    .setInputCols([\"document\"])    .setOutputCol(\"sentence_embeddings\")\n\n# Configure the SentimentDLModel\nsentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n\n# Set up the NLP Pipeline\nnlpPipeline = Pipeline(\n    stages=[\n        documentAssembler,\n        use,\n        sentimentdl\n    ])\n\n# Apply the Pipeline to your DataFrame\npipelineModel = nlpPipeline.fit(comment_load)\nresults = pipel...",
                    "submissionTime": "2023-11-13T23:45:26.773GMT",
                    "completionTime": "2023-11-13T23:45:26.974GMT",
                    "stageIds": [
                      6
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 4,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 4,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 4,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at Feature.scala:182",
                    "dataWritten": 0,
                    "dataRead": 95,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "first at Feature.scala:182",
                    "description": "Job group for statement 11:\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder, SentimentDLModel\nfrom pyspark.ml import Pipeline\n\n# Define the name of the SentimentDLModel\nMODEL_NAME = \"sentimentdl_use_twitter\"  # Replace with the model name you intend to use\n\n# Configure the Document Assembler\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"comment_text\")    .setOutputCol(\"document\")\n\n# Configure the Universal Sentence Encoder\nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")    .setInputCols([\"document\"])    .setOutputCol(\"sentence_embeddings\")\n\n# Configure the SentimentDLModel\nsentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n\n# Set up the NLP Pipeline\nnlpPipeline = Pipeline(\n    stages=[\n        documentAssembler,\n        use,\n        sentimentdl\n    ])\n\n# Apply the Pipeline to your DataFrame\npipelineModel = nlpPipeline.fit(comment_load)\nresults = pipel...",
                    "submissionTime": "2023-11-13T23:45:26.628GMT",
                    "completionTime": "2023-11-13T23:45:26.753GMT",
                    "stageIds": [
                      5
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 463,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 11:\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder, SentimentDLModel\nfrom pyspark.ml import Pipeline\n\n# Define the name of the SentimentDLModel\nMODEL_NAME = \"sentimentdl_use_twitter\"  # Replace with the model name you intend to use\n\n# Configure the Document Assembler\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"comment_text\")    .setOutputCol(\"document\")\n\n# Configure the Universal Sentence Encoder\nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")    .setInputCols([\"document\"])    .setOutputCol(\"sentence_embeddings\")\n\n# Configure the SentimentDLModel\nsentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n\n# Set up the NLP Pipeline\nnlpPipeline = Pipeline(\n    stages=[\n        documentAssembler,\n        use,\n        sentimentdl\n    ])\n\n# Apply the Pipeline to your DataFrame\npipelineModel = nlpPipeline.fit(comment_load)\nresults = pipel...",
                    "submissionTime": "2023-11-13T23:45:26.258GMT",
                    "completionTime": "2023-11-13T23:45:26.458GMT",
                    "stageIds": [
                      4
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 346,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 11:\nfrom sparknlp.base import DocumentAssembler\nfrom sparknlp.annotator import UniversalSentenceEncoder, SentimentDLModel\nfrom pyspark.ml import Pipeline\n\n# Define the name of the SentimentDLModel\nMODEL_NAME = \"sentimentdl_use_twitter\"  # Replace with the model name you intend to use\n\n# Configure the Document Assembler\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"comment_text\")    .setOutputCol(\"document\")\n\n# Configure the Universal Sentence Encoder\nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")    .setInputCols([\"document\"])    .setOutputCol(\"sentence_embeddings\")\n\n# Configure the SentimentDLModel\nsentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n\n# Set up the NLP Pipeline\nnlpPipeline = Pipeline(\n    stages=[\n        documentAssembler,\n        use,\n        sentimentdl\n    ])\n\n# Apply the Pipeline to your DataFrame\npipelineModel = nlpPipeline.fit(comment_load)\nresults = pipel...",
                    "submissionTime": "2023-11-13T23:44:23.684GMT",
                    "completionTime": "2023-11-13T23:44:23.881GMT",
                    "stageIds": [
                      3
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "b452f245-0c16-437a-b1bd-b9eae8cbece4"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n[OK!]\nsentimentdl_use_twitter download started this may take some time.\nApproximate size to download 11.4 MB\n[OK!]\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699919132214
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 20,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:57:06.1110257Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:57:06.2911496Z",
              "execution_finish_time": "2023-11-13T22:57:06.5905849Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "94da2086-4050-4abd-ae3a-9d3d6108d4f6"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 20, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "root\n |-- sub_id: string (nullable = true)\n |-- comment_author: string (nullable = true)\n |-- comment_text: string (nullable = true)\n |-- link_id: string (nullable = true)\n |-- comment_score: long (nullable = true)\n |-- comment_controversiality: long (nullable = true)\n |-- reply_to: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- score_category: string (nullable = true)\n |-- document: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = false)\n |    |    |-- end: integer (nullable = false)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = false)\n |-- sentence_embeddings: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = false)\n |    |    |-- end: integer (nullable = false)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = false)\n |-- sentiment: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = false)\n |    |    |-- end: integer (nullable = false)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = false)\n\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699916227481
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = results.select(\"comment_text\",\"comment_controversiality\",\"reply_to\",\"score_category\",F.explode(\"sentiment.result\").alias(\"sentiment\"))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:45:46.1015496Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:45:46.2702404Z",
              "execution_finish_time": "2023-11-13T23:45:55.7526012Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "98e7722f-a5e2-4dc2-8ab5-a9174d1601bd"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 12, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699919155912
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.show(10)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "12",
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T22:59:41.1735844Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T22:59:41.308212Z",
              "execution_finish_time": "2023-11-13T23:00:04.3252452Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2502440,
                    "rowCount": 2,
                    "usageDescription": "",
                    "jobId": 25,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 23:\nresult_df.show(10)",
                    "submissionTime": "2023-11-13T22:59:41.791GMT",
                    "completionTime": "2023-11-13T23:00:04.056GMT",
                    "stageIds": [
                      42
                    ],
                    "jobGroup": "23",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "f906d36a-0230-4844-9801-f0d7b1231982"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 12, 23, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------------+------------------------+--------+--------------+---------+\n|        comment_text|comment_controversiality|reply_to|score_category|sentiment|\n+--------------------+------------------------+--------+--------------+---------+\n|yes it feels like...|                       0|      t3|     Very High| positive|\n|   Hahaha! What?????|                       0|      t1|           Low| positive|\n|           [deleted]|                       0|      t1|        Medium| negative|\n|I'd photo my frie...|                       0|      t3|           Low| positive|\n|I think you may b...|                       0|      t1|           Low| positive|\n|&gt;they're reall...|                       0|      t1|        Medium| positive|\n|Interesting why d...|                       0|      t1|           Low| positive|\n|I did some creepy...|                       0|      t3|           Low| negative|\n|I see Shrek using...|                       0|      t1|           Low| negative|\n|                isfp|                       0|      t3|          High| positive|\n+--------------------+------------------------+--------+--------------+---------+\nonly showing top 10 rows\n\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699916404780
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.limit(5).toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/sentiment_result_limit5.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:59:46.2610015Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:59:46.4292057Z",
              "execution_finish_time": "2023-11-13T23:59:56.2514468Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_18765/1360916113.py:1",
                    "dataWritten": 224634,
                    "dataRead": 332637790,
                    "rowCount": 1408,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "toPandas at /tmp/ipykernel_18765/1360916113.py:1",
                    "description": "Job group for statement 14:\nresult_df.limit(5).toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/sentiment_result_limit5.csv\",index=False)",
                    "submissionTime": "2023-11-13T23:59:47.383GMT",
                    "completionTime": "2023-11-13T23:59:54.026GMT",
                    "stageIds": [
                      10,
                      11
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 234,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 234,
                    "numActiveStages": 0,
                    "numCompletedStages": 2,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "a34a9c1b-f8fc-47ed-9434-641427d1b28c"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 14, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699919996570
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Group by score_category and sentiment, then count the occurrences\n",
        "category_sentiment_counts = result_df.groupBy(\"score_category\", \"sentiment\").count()\n",
        "category_sentiment_counts.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-13T23:45:59.4131844Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-13T23:45:59.5393082Z",
              "execution_finish_time": "2023-11-13T23:58:45.7811914Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 249783,
                    "rowCount": 2796,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\nfrom pyspark.sql import functions as F\n\n# Group by score_category and sentiment, then count the occurrences\ncategory_sentiment_counts = result_df.groupBy(\"score_category\", \"sentiment\").count()\ncategory_sentiment_counts.show()",
                    "submissionTime": "2023-11-13T23:58:43.907GMT",
                    "completionTime": "2023-11-13T23:58:44.156GMT",
                    "stageIds": [
                      9,
                      8
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 233,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 249783,
                    "dataRead": 333762496,
                    "rowCount": 3039,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\nfrom pyspark.sql import functions as F\n\n# Group by score_category and sentiment, then count the occurrences\ncategory_sentiment_counts = result_df.groupBy(\"score_category\", \"sentiment\").count()\ncategory_sentiment_counts.show()",
                    "submissionTime": "2023-11-13T23:46:00.881GMT",
                    "completionTime": "2023-11-13T23:58:43.575GMT",
                    "stageIds": [
                      7
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "f52ceb60-61b2-4bd5-8554-5911e621a0ce"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+---------+------+\n|score_category|sentiment| count|\n+--------------+---------+------+\n|           Low|  neutral| 43229|\n|           Low| negative|257820|\n|        Medium| positive|288381|\n|     Very High| positive|274252|\n|           Low| positive|509866|\n|          High| negative| 50260|\n|        Medium| negative|105302|\n|     Very High| negative|118009|\n|        Medium|  neutral| 22645|\n|     Very High|  neutral| 23356|\n|          High|  neutral| 10537|\n|          High| positive|130481|\n+--------------+---------+------+\n\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699919926036
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_sentiment_counts.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/sentiment_counts.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-14T00:02:11.1018012Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-14T00:02:11.2330853Z",
              "execution_finish_time": "2023-11-14T00:14:20.3956697Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_18765/4182414353.py:1",
                    "dataWritten": 0,
                    "dataRead": 249783,
                    "rowCount": 2796,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "toPandas at /tmp/ipykernel_18765/4182414353.py:1",
                    "description": "Job group for statement 15:\ncategory_sentiment_counts.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/sentiment_counts.csv\",index=False)",
                    "submissionTime": "2023-11-14T00:14:17.622GMT",
                    "completionTime": "2023-11-14T00:14:17.765GMT",
                    "stageIds": [
                      13,
                      14
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 233,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_18765/4182414353.py:1",
                    "dataWritten": 249783,
                    "dataRead": 333762496,
                    "rowCount": 3039,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "toPandas at /tmp/ipykernel_18765/4182414353.py:1",
                    "description": "Job group for statement 15:\ncategory_sentiment_counts.toPandas().to_csv(\"Users/ml2078/fall-2023-reddit-project-team-10/data/csv/sentiment_counts.csv\",index=False)",
                    "submissionTime": "2023-11-14T00:02:11.461GMT",
                    "completionTime": "2023-11-14T00:14:17.533GMT",
                    "stageIds": [
                      12
                    ],
                    "jobGroup": "15",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "cfea3429-48a5-4fc2-ab05-bcbc34250fd8"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 15, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699920860580
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = category_sentiment_counts.toPandas()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-14T00:15:35.2890199Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-14T00:15:35.4360771Z",
              "execution_finish_time": "2023-11-14T00:27:40.143899Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_18765/2532664331.py:1",
                    "dataWritten": 0,
                    "dataRead": 249783,
                    "rowCount": 2796,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "toPandas at /tmp/ipykernel_18765/2532664331.py:1",
                    "description": "Job group for statement 16:\npandas_df = category_sentiment_counts.toPandas()\n\n# Reshape the data for heatmap plotting\nheatmap_data = pandas_df.pivot(\"score_category\", \"sentiment\", \"count\")",
                    "submissionTime": "2023-11-14T00:27:38.865GMT",
                    "completionTime": "2023-11-14T00:27:39.359GMT",
                    "stageIds": [
                      16,
                      17
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 234,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 233,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 1,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "toPandas at /tmp/ipykernel_18765/2532664331.py:1",
                    "dataWritten": 249783,
                    "dataRead": 333762496,
                    "rowCount": 3039,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "toPandas at /tmp/ipykernel_18765/2532664331.py:1",
                    "description": "Job group for statement 16:\npandas_df = category_sentiment_counts.toPandas()\n\n# Reshape the data for heatmap plotting\nheatmap_data = pandas_df.pivot(\"score_category\", \"sentiment\", \"count\")",
                    "submissionTime": "2023-11-14T00:15:35.676GMT",
                    "completionTime": "2023-11-14T00:27:38.803GMT",
                    "stageIds": [
                      15
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 233,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 233,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 233,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "f947efe0-f6c5-40c5-bfe3-ef025e6f310f"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 16, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699921660429
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the data for heatmap plotting\n",
        "heatmap_data = pandas_df.pivot(\"score_category\", \"sentiment\", \"count\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 30,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-14T00:42:55.4924289Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-14T00:42:55.6342385Z",
              "execution_finish_time": "2023-11-14T00:42:55.9282836Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d2a6de29-b078-4cd1-870a-b9e9b1c88ff9"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 30, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699922576180
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Convert the 'score_category' to a categorical type with the desired order\n",
        "ordered_categories = ['Low','Medium','High','Very High']\n",
        "heatmap_data.index = pd.CategoricalIndex(heatmap_data.index, categories=ordered_categories, ordered=True)\n",
        "\n",
        "# Sort the DataFrame by the 'score_category' index to ensure the order is applied\n",
        "heatmap_data.sort_index(level='score_category', ascending=False, inplace=True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-14T00:42:57.4618676Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-14T00:42:57.5993661Z",
              "execution_finish_time": "2023-11-14T00:42:57.8853284Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "712080e2-5d3f-4faa-8e22-a18dbe52557d"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 31, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699922578070
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap_data.head()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 32,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-14T00:42:58.9994802Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-14T00:42:59.1019176Z",
              "execution_finish_time": "2023-11-14T00:42:59.3912846Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "0d777fd4-4e00-4fd0-8603-ccba218ae3ac"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 32, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 63,
          "data": {
            "text/plain": "sentiment       negative  neutral  positive\nscore_category                             \nVery High         118009    23356    274252\nHigh               50260    10537    130481\nMedium            105302    22645    288381\nLow               257820    43229    509866",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>sentiment</th>\n      <th>negative</th>\n      <th>neutral</th>\n      <th>positive</th>\n    </tr>\n    <tr>\n      <th>score_category</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Very High</th>\n      <td>118009</td>\n      <td>23356</td>\n      <td>274252</td>\n    </tr>\n    <tr>\n      <th>High</th>\n      <td>50260</td>\n      <td>10537</td>\n      <td>130481</td>\n    </tr>\n    <tr>\n      <th>Medium</th>\n      <td>105302</td>\n      <td>22645</td>\n      <td>288381</td>\n    </tr>\n    <tr>\n      <th>Low</th>\n      <td>257820</td>\n      <td>43229</td>\n      <td>509866</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699922579745
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sentiment_heatmap = sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
        "plt.title('Heatmap of Sentiment Percentage by Score Category')\n",
        "plt.ylabel('Score Category')\n",
        "plt.xlabel('Sentiment')\n",
        "\n",
        "plt.savefig('Users/ml2078/fall-2023-reddit-project-team-10/plots/csv/heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "80dcc4b2-bc50-4e81-91e5-397b7f13252e",
              "session_id": "15",
              "statement_id": 34,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-14T00:44:40.3019643Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-14T00:44:40.51504Z",
              "execution_finish_time": "2023-11-14T00:44:41.3350924Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "93933796-396b-49a5-a06e-9fd3612827d8"
            },
            "text/plain": "StatementMeta(80dcc4b2-bc50-4e81-91e5-397b7f13252e, 15, 34, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Users/ml2078/fall-2023-reddit-project-team-10/plots/csv/heatmap.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_18765/1598117426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Users/ml2078/fall-2023-reddit-project-team-10/plots/csv/heatmap.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3003\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3005\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \"\"\"\n\u001b[1;32m    508\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1616\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2167\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2169\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Users/ml2078/fall-2023-reddit-project-team-10/plots/csv/heatmap.png'"
          ]
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699922681518
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}